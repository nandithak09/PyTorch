{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1067156,"sourceType":"datasetVersion","datasetId":592212}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-05T06:35:30.698683Z","iopub.execute_input":"2025-11-05T06:35:30.699004Z","iopub.status.idle":"2025-11-05T06:35:30.961350Z","shell.execute_reply.started":"2025-11-05T06:35:30.698971Z","shell.execute_reply":"2025-11-05T06:35:30.960622Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/language-translation-englishfrench/eng_-french.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install evaluate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:51:52.569169Z","iopub.execute_input":"2025-11-05T07:51:52.569453Z","iopub.status.idle":"2025-11-05T07:52:01.177894Z","shell.execute_reply.started":"2025-11-05T07:51:52.569432Z","shell.execute_reply":"2025-11-05T07:52:01.177093Z"}},"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.1.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.4.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.5)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.9.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.0.0rc2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.19.1)\nCollecting pyarrow>=21.0.0 (from datasets>=2.0.0->evaluate)\n  Downloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.3)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.15)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.28.1)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (0.19.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.15.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.10)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.3)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.8.3)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.4)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (4.11.0)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (0.16.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer-slim->huggingface-hub>=0.7.0->evaluate) (8.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->huggingface-hub>=0.7.0->evaluate) (1.3.1)\nDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-22.0.0-cp311-cp311-manylinux_2_28_x86_64.whl (47.7 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m38.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, evaluate\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 19.0.1\n    Uninstalling pyarrow-19.0.1:\n      Successfully uninstalled pyarrow-19.0.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed evaluate-0.4.6 pyarrow-22.0.0\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:52:02.582163Z","iopub.execute_input":"2025-11-05T07:52:02.582867Z","iopub.status.idle":"2025-11-05T07:52:06.496154Z","shell.execute_reply.started":"2025-11-05T07:52:02.582839Z","shell.execute_reply":"2025-11-05T07:52:06.495290Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2025.9.18)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.4.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.5.1\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install -U transformers==4.45.2 datasets==2.20.0 pyarrow==15.0.2 evaluate==0.4.2 sacrebleu==2.4.0 --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:52:09.150652Z","iopub.execute_input":"2025-11-05T07:52:09.151465Z","iopub.status.idle":"2025-11-05T07:52:27.985503Z","shell.execute_reply.started":"2025-11-05T07:52:09.151431Z","shell.execute_reply":"2025-11-05T07:52:27.984471Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.4/57.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m547.8/547.8 kB\u001b[0m \u001b[31m34.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m316.1/316.1 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.5.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip uninstall -y pyarrow datasets\n!pip install pyarrow==15.0.2 datasets==2.20.0 --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:52:27.987238Z","iopub.execute_input":"2025-11-05T07:52:27.987582Z","iopub.status.idle":"2025-11-05T07:52:35.382667Z","shell.execute_reply.started":"2025-11-05T07:52:27.987546Z","shell.execute_reply":"2025-11-05T07:52:35.381819Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: pyarrow 15.0.2\nUninstalling pyarrow-15.0.2:\n  Successfully uninstalled pyarrow-15.0.2\nFound existing installation: datasets 2.20.0\nUninstalling datasets-2.20.0:\n  Successfully uninstalled datasets-2.20.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom datasets import Dataset, DatasetDict\nimport evaluate\nimport sacrebleu\nimport torch\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSeq2SeqLM,\n    DataCollatorForSeq2Seq,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer,\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:52:43.564211Z","iopub.execute_input":"2025-11-05T07:52:43.564845Z","iopub.status.idle":"2025-11-05T07:53:09.558411Z","shell.execute_reply.started":"2025-11-05T07:52:43.564814Z","shell.execute_reply":"2025-11-05T07:53:09.557519Z"}},"outputs":[{"name":"stderr","text":"2025-11-05 07:52:53.819854: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762329174.020797      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762329174.078295      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# ---------------------------\n# Config\n# ---------------------------\nDATA_PATH = \"/kaggle/input/language-translation-englishfrench/eng_-french.csv\"  # Update path if needed\nMODEL_NAME = \"t5-small\"     # You can try 't5-base' if you have GPU memory\nOUTPUT_DIR = \"./t5_en_fr_checkpoints\"\nMAX_INPUT_LENGTH = 128\nMAX_TARGET_LENGTH = 128\nBATCH_SIZE = 8\nEPOCHS = 3\nLEARNING_RATE = 5e-5\nSEED = 42\n\ntorch.manual_seed(SEED)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:53:10.155088Z","iopub.execute_input":"2025-11-05T07:53:10.155299Z","iopub.status.idle":"2025-11-05T07:53:10.167741Z","shell.execute_reply.started":"2025-11-05T07:53:10.155283Z","shell.execute_reply":"2025-11-05T07:53:10.167001Z"}},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7f952f38e1d0>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict\n\nSRC_COLS = [\"english\", \"en\", \"source\", \"src\", \"eng\", \"English\"]\nTGT_COLS = [\"french\", \"fr\", \"target\", \"tgt\", \"fra\", \"French\"]\n\ndef detect_columns(df_columns):\n\n    lowercols = [c.lower() for c in df_columns]\n    src_col, tgt_col = None, None\n    for c in SRC_COLS:\n        if c.lower() in lowercols:\n            src_col = df_columns[lowercols.index(c.lower())]\n            break\n    for c in TGT_COLS:\n        if c.lower() in lowercols:\n            tgt_col = df_columns[lowercols.index(c.lower())]\n            break\n    return src_col, tgt_col\n\n\ndef read_csv_to_dataset(path: str, src_col=None, tgt_col=None):\n    df = pd.read_csv(path)\n    if src_col is None or tgt_col is None:\n        detected_src, detected_tgt = detect_columns(list(df.columns))\n        if not detected_src or not detected_tgt:\n            raise ValueError(f\"Could not detect src/tgt columns. Found: {list(df.columns)}\")\n        src_col, tgt_col = detected_src, detected_tgt\n\n    df = df[[src_col, tgt_col]].rename(columns={src_col: \"src\", tgt_col: \"tgt\"})\n    df = df.dropna().reset_index(drop=True)\n\n    # Split 80/10/10\n    n = len(df)\n    train_end, valid_end = int(0.8 * n), int(0.9 * n)\n    train_df, valid_df, test_df = df[:train_end], df[train_end:valid_end], df[valid_end:]\n\n    ds = DatasetDict({\n        \"train\": Dataset.from_pandas(train_df),\n        \"validation\": Dataset.from_pandas(valid_df),\n        \"test\": Dataset.from_pandas(test_df),\n    })\n    return ds\n\n\n# Load dataset\ndataset = read_csv_to_dataset(\n    DATA_PATH,\n    src_col='English words/sentences',\n    tgt_col='French words/sentences'\n)\n\nprint(\"âœ… Dataset loaded successfully!\")\nprint(\"Sizes:\", {k: len(v) for k, v in dataset.items()})\ndataset[\"train\"][0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T08:00:23.723401Z","iopub.execute_input":"2025-11-05T08:00:23.723680Z","iopub.status.idle":"2025-11-05T08:00:24.234713Z","shell.execute_reply.started":"2025-11-05T08:00:23.723658Z","shell.execute_reply":"2025-11-05T08:00:24.234036Z"}},"outputs":[{"name":"stdout","text":"âœ… Dataset loaded successfully!\nSizes: {'train': 140496, 'validation': 17562, 'test': 17563}\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"{'src': 'Hi.', 'tgt': 'Salut!'}"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nMODEL_NAME = \"t5-small\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\nprint(\"âœ… Model and tokenizer loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:53:21.318045Z","iopub.execute_input":"2025-11-05T07:53:21.318301Z","iopub.status.idle":"2025-11-05T07:53:25.023649Z","shell.execute_reply.started":"2025-11-05T07:53:21.318284Z","shell.execute_reply":"2025-11-05T07:53:25.022866Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.32k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"907b8c08253b4af2bf7f91fef0b3b29c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"41ddd2aeebd1481da54ef1df965382f3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"30d35edd77e145e2812ddd32eb696e9f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c28ea19dfc844eddaa68a05f82d7e6f5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/242M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1363044f8c4b4101bbcd7d1a8aa5b598"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19189426292641adb7ae2f563325dee9"}},"metadata":{}},{"name":"stdout","text":"âœ… Model and tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nMODEL_NAME = \"t5-small\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=False, local_files_only=False)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, trust_remote_code=False, local_files_only=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:53:28.396903Z","iopub.execute_input":"2025-11-05T07:53:28.398000Z","iopub.status.idle":"2025-11-05T07:53:29.377547Z","shell.execute_reply.started":"2025-11-05T07:53:28.397946Z","shell.execute_reply":"2025-11-05T07:53:29.376662Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"!pip install -U pip\n!pip install transformers==4.45.2 huggingface_hub==0.25.2 datasets==3.1.0 pyarrow==15.0.2 --quiet\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:53:38.551476Z","iopub.execute_input":"2025-11-05T07:53:38.552133Z","iopub.status.idle":"2025-11-05T07:53:47.496307Z","shell.execute_reply.started":"2025-11-05T07:53:38.552108Z","shell.execute_reply":"2025-11-05T07:53:47.495260Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\nCollecting pip\n  Downloading pip-25.3-py3-none-any.whl.metadata (4.7 kB)\nDownloading pip-25.3-py3-none-any.whl (1.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pip\n  Attempting uninstall: pip\n    Found existing installation: pip 24.1.2\n    Uninstalling pip-24.1.2:\n      Successfully uninstalled pip-24.1.2\nSuccessfully installed pip-25.3\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndiffusers 0.34.0 requires huggingface-hub>=0.27.0, but you have huggingface-hub 0.25.2 which is incompatible.\ngradio 5.38.1 requires huggingface-hub>=0.28.1, but you have huggingface-hub 0.25.2 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nMODEL_NAME = \"t5-small\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\nprint(\"âœ… Model and tokenizer loaded successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:53:47.498262Z","iopub.execute_input":"2025-11-05T07:53:47.498663Z","iopub.status.idle":"2025-11-05T07:53:48.118128Z","shell.execute_reply.started":"2025-11-05T07:53:47.498637Z","shell.execute_reply":"2025-11-05T07:53:48.117358Z"}},"outputs":[{"name":"stdout","text":"âœ… Model and tokenizer loaded successfully!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# Load tokenizer and model\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n\nprefix = \"translate English to French: \"\n\ndef preprocess_function(examples):\n    inputs = [prefix + s for s in examples[\"src\"]]\n    model_inputs = tokenizer(inputs, max_length=MAX_INPUT_LENGTH, truncation=True, padding=\"max_length\")\n\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(examples[\"tgt\"], max_length=MAX_TARGET_LENGTH, truncation=True, padding=\"max_length\")\n\n    labels_input_ids = [[(id if id != tokenizer.pad_token_id else -100) for id in l] for l in labels[\"input_ids\"]]\n    model_inputs[\"labels\"] = labels_input_ids\n    return model_inputs\n\n\nprint(\"ğŸ”„ Tokenizing dataset...\")\ntokenized_datasets = dataset.map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\ntokenized_datasets.set_format(type=\"torch\")\nprint(\"Tokenized columns:\", tokenized_datasets[\"train\"].column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T07:53:56.796999Z","iopub.execute_input":"2025-11-05T07:53:56.797782Z","iopub.status.idle":"2025-11-05T07:54:37.004104Z","shell.execute_reply.started":"2025-11-05T07:53:56.797760Z","shell.execute_reply":"2025-11-05T07:54:37.003382Z"}},"outputs":[{"name":"stdout","text":"ğŸ”„ Tokenizing dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/140496 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e453bcf228f4081b18e57a42869ffe2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:4109: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17562 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a19cb212fd542e5ab4639be535c2275"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17563 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2cade89b94e24c08b6ea5286dfbd9266"}},"metadata":{}},{"name":"stdout","text":"Tokenized columns: ['input_ids', 'attention_mask', 'labels']\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"!pip install sacrebleu evaluate -q\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T08:02:56.230385Z","iopub.execute_input":"2025-11-05T08:02:56.231060Z","iopub.status.idle":"2025-11-05T08:02:58.451987Z","shell.execute_reply.started":"2025-11-05T08:02:56.231001Z","shell.execute_reply":"2025-11-05T08:02:58.451161Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"from transformers import DataCollatorForSeq2Seq\nimport numpy as np\nimport sacrebleu\n\n# Data collator remains the same\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n\n# Define text cleaning\ndef postprocess_text(preds, labels):\n    preds = [p.strip() for p in preds]\n    labels = [[l.strip()] for l in labels]  # sacrebleu expects list of list\n    return preds, labels\n\n# Custom BLEU computation without evaluate\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n\n    # Decode predictions and labels\n    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n\n    # Clean and prepare for BLEU computation\n    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n\n    # Compute BLEU score directly using sacrebleu\n    bleu = sacrebleu.corpus_bleu(decoded_preds, decoded_labels)\n\n    return {\"bleu\": bleu.score}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T08:05:17.060126Z","iopub.execute_input":"2025-11-05T08:05:17.060856Z","iopub.status.idle":"2025-11-05T08:05:17.066612Z","shell.execute_reply.started":"2025-11-05T08:05:17.060832Z","shell.execute_reply":"2025-11-05T08:05:17.065683Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import torch\nfrom transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\nfrom tqdm import tqdm\nimport os\n\n# Ensure GPU is used\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\" Using device: {device.upper()}\")\n\nmodel.to(device)\n\ntraining_args = Seq2SeqTrainingArguments(\n    output_dir=OUTPUT_DIR,\n    evaluation_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=LEARNING_RATE,\n    per_device_train_batch_size=BATCH_SIZE,\n    per_device_eval_batch_size=BATCH_SIZE,\n    weight_decay=0.01,\n    save_total_limit=2,\n    num_train_epochs=EPOCHS,\n    predict_with_generate=True,\n    fp16=torch.cuda.is_available(),  # use mixed precision on GPU\n    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n    logging_strategy=\"steps\",\n    logging_steps=50,  # log more frequently for tqdm\n    load_best_model_at_end=True,\n    metric_for_best_model=\"bleu\",\n    greater_is_better=True,\n    report_to=\"none\"  # disable wandb/tensorboard noise\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics,\n)\n\nprint(\" Starting fine-tuning...\")\n\n# Run training with tqdm progress bar\ntrain_output = trainer.train()\nprint(\"Training complete!\")\n\n# Evaluate model\neval_results = trainer.evaluate()\nprint(\"ğŸ“Š Evaluation results:\", eval_results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T08:10:20.884664Z","iopub.execute_input":"2025-11-05T08:10:20.885309Z","iopub.status.idle":"2025-11-05T10:09:32.216200Z","shell.execute_reply.started":"2025-11-05T08:10:20.885284Z","shell.execute_reply":"2025-11-05T10:09:32.215349Z"}},"outputs":[{"name":"stdout","text":"ğŸ”¥ Using device: CUDA\nğŸš€ Starting fine-tuning...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/data/data_collator.py:656: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:254.)\n  batch[\"labels\"] = torch.tensor(batch[\"labels\"], dtype=torch.int64)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='26343' max='26343' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [26343/26343 1:53:26, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Bleu</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.679600</td>\n      <td>0.577231</td>\n      <td>34.926710</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.612900</td>\n      <td>0.554946</td>\n      <td>34.926710</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.620500</td>\n      <td>0.549123</td>\n      <td>34.926710</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py:1220: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"âœ… Training complete!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1098' max='1098' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1098/1098 05:40]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"ğŸ“Š Evaluation results: {'eval_loss': 0.5772307515144348, 'eval_bleu': 34.926710282900494, 'eval_runtime': 342.6891, 'eval_samples_per_second': 51.248, 'eval_steps_per_second': 3.204, 'epoch': 3.0}\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"def translate_sentence(sentence, model, tokenizer, max_length=64):\n    model.eval()  # put model in evaluation mode\n    prefix = \"translate English to French: \"\n    input_text = prefix + sentence\n\n    # Tokenize input\n    inputs = tokenizer(\n        input_text,\n        return_tensors=\"pt\",\n        truncation=True,\n        padding=True,\n        max_length=max_length\n    ).to(model.device) \n\n    # Generate translation\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=max_length,\n            num_beams=5,  # beam search for better quality\n            early_stopping=True\n        )\n\n    # Decode output tokens\n    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return translated_text\n\n\n#  Test the model with a few examples\ntest_sentences = [\n    \"The weather is very nice today.\",\n    \"I love learning new languages.\",\n    \"The cat is sleeping on the sofa.\",\n    \"Where is the nearest train station?\"\n]\n\nprint(\"\\n English â†’  French Translations:\")\nfor sent in test_sentences:\n    french = translate_sentence(sent, model, tokenizer)\n    print(f\"\\n English: {sent}\\n French:  {french}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-05T10:10:29.766387Z","iopub.execute_input":"2025-11-05T10:10:29.766776Z","iopub.status.idle":"2025-11-05T10:10:30.673311Z","shell.execute_reply.started":"2025-11-05T10:10:29.766749Z","shell.execute_reply":"2025-11-05T10:10:30.672438Z"}},"outputs":[{"name":"stdout","text":"\n English â†’  French Translations:\n\n English: The weather is very nice today.\n French:  Le temps est trÃ¨s agrÃ©able aujourd'hui.\n\n English: I love learning new languages.\n French:  J'adore apprendre de nouvelles langues.\n\n English: The cat is sleeping on the sofa.\n French:  Le chat dorme sur le canapÃ©.\n\n English: Where is the nearest train station?\n French:  OÃ¹ se trouve la gare la plus proche?\n","output_type":"stream"}],"execution_count":32}]}