{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1067156,"sourceType":"datasetVersion","datasetId":592212}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport pandas as pd\nimport numpy as np\nimport torch.optim as optim\nimport re\nimport math\nimport time\nfrom torch.utils.data import Dataset, DataLoader\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"using device :{device}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T06:33:57.298521Z","iopub.execute_input":"2025-11-20T06:33:57.298677Z","iopub.status.idle":"2025-11-20T06:34:02.531038Z","shell.execute_reply.started":"2025-11-20T06:33:57.298663Z","shell.execute_reply":"2025-11-20T06:34:02.530234Z"}},"outputs":[{"name":"stdout","text":"using device :cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"df = pd. read_csv(\"/kaggle/input/language-translation-englishfrench/eng_-french.csv\")\ndf = df.rename(columns={'English words/sentences': 'english', 'French words/sentences': 'french'})\n\nprint(f\"Dataset Size: {len(df)}\")\nprint(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T06:34:06.105844Z","iopub.execute_input":"2025-11-20T06:34:06.106128Z","iopub.status.idle":"2025-11-20T06:34:06.641206Z","shell.execute_reply.started":"2025-11-20T06:34:06.106098Z","shell.execute_reply":"2025-11-20T06:34:06.640561Z"}},"outputs":[{"name":"stdout","text":"Dataset Size: 175621\n  english      french\n0     Hi.      Salut!\n1    Run!     Cours !\n2    Run!    Courez !\n3    Who?       Qui ?\n4    Wow!  Ça alors !\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"class Vocabulary:\n    def __init__(self):\n        self.word2idx = {'<pad>':0,'<sos>':1, '<eos>':2,'<unk>':3}\n        self.idx2word = {0:'<pad>',1:'<sos>',2:'<eos>',3:'<unk>'}\n        self.vocab_size = 4\n\n    def build_vocab(self, sentences, min_freq = 2):\n        word_freq={}\n\n        for sentence in sentences:\n            for word in sentence.split():\n                word_freq[word]= word_freq.get(word,0)+1\n\n        for word,freq in word_freq.items():\n            if freq >= min_freq:\n                self.word2idx[word]= self.vocab_size\n                self.idx2word[self.vocab_size]=word\n                self.vocab_size+=1\n                \n    def sentence_to_indices(self,sentence):\n        indices = [self.word2idx.get(word, self.word2idx['<unk>']) for word in sentence.split()]\n        return indices\n\n    def indices_to_sentence(self,indices):\n        word = [self.idx2word.get(idx, self.idx2word['<unk>'])for idx in indices]\n        return ' '.join(word)\n\ndef preprocess_text(text):\n    text = text.lower()\n    text = re.sub(r\"([.!?])\", r\" \\1\", text)\n    text = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", text)\n    return text.strip()\n\ndf['english'] = df['english'].apply(preprocess_text)\ndf['french'] = df['french'].apply(preprocess_text)\n\neng_vocab = Vocabulary()\nfr_vocab = Vocabulary()\n\neng_vocab.build_vocab(df['english'])\nfr_vocab.build_vocab(df['french'])\n\nprint(f\"English Vocabulary size : {eng_vocab.vocab_size}\")\nprint(f\"French Vocabulary size : {fr_vocab.vocab_size}\")       ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T06:34:11.486147Z","iopub.execute_input":"2025-11-20T06:34:11.486874Z","iopub.status.idle":"2025-11-20T06:34:13.977913Z","shell.execute_reply.started":"2025-11-20T06:34:11.486848Z","shell.execute_reply":"2025-11-20T06:34:13.977178Z"}},"outputs":[{"name":"stdout","text":"English Vocabulary size : 9680\nFrench Vocabulary size : 13376\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n    def __init__(self,d_model,num_head, dropout = 0.1):\n        super(MultiheadAttention,self).__init__()\n        assert d_model%num_head == 0\n        \n        self.d_model = d_model\n        self.num_head = num_head\n        self.dim_head = d_model//num_head\n        \n        self.w_q = nn.Linear(d_model,d_model)\n        self.w_k = nn.Linear(d_model,d_model)\n        self.w_v = nn.Linear(d_model,d_model)\n        self.w_o = nn.Linear(d_model,d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.scale = torch.sqrt(torch.FloatTensor([self.dim_head])).to(device)\n\n    def forward(self,query,key,value,mask=None):\n        batch_size = query.shape[0]\n        Q = self.w_q(query)\n        K = self.w_k(key)\n        V = self.w_v(value)\n\n        Q = Q.view(batch_size, -1, self.num_head, self.dim_head).transpose(1,2)\n        K = K.view(batch_size , -1, self.num_head,self.dim_head).transpose(1,2)\n        V = V.view(batch_size,-1,self.num_head,self.dim_head).transpose(1,2)\n\n        scores = torch.matmul(Q,K.transpose(-2,-1))/self.scale\n        \n        if mask is not None:\n            scores = scores.masked_fill(mask==0, -1e9)\n\n        attention = torch.softmax(scores,dim=-1)\n        attention = self.dropout(attention)\n\n        x = torch.matmul(attention,V)\n        \n        x = x.transpose(1,2).contiguous().view(batch_size,-1,self.d_model)\n        x = self.w_o(x)\n        return x, attention","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:42:55.773076Z","iopub.execute_input":"2025-11-20T07:42:55.773567Z","iopub.status.idle":"2025-11-20T07:42:55.780904Z","shell.execute_reply.started":"2025-11-20T07:42:55.773545Z","shell.execute_reply":"2025-11-20T07:42:55.780042Z"}},"outputs":[],"execution_count":87},{"cell_type":"code","source":"class PositionwiseFeedForward(nn.Module):\n    def __init__(self,d_model,dff,dropout = 0.1):\n        super(PositionwiseFeedForward,self).__init__()\n        self.linear1 = nn.Linear(d_model,dff)\n        self.linear2 = nn.Linear(dff,d_model)\n        self.dropout = nn.Dropout(dropout)\n        self.activation = nn.ReLU()\n\n    def forward(self,x):\n        x = self.linear1(x)\n        x = self.activation(x)\n        x = self.dropout(x)\n        x = self.linear2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:42:59.596450Z","iopub.execute_input":"2025-11-20T07:42:59.597051Z","iopub.status.idle":"2025-11-20T07:42:59.601732Z","shell.execute_reply.started":"2025-11-20T07:42:59.597025Z","shell.execute_reply":"2025-11-20T07:42:59.600976Z"}},"outputs":[],"execution_count":88},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self,d_model, num_head,dff, dropout=0.1):\n        super(EncoderLayer,self).__init__()\n        self.selfAttention = MultiheadAttention(d_model,num_head,dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model,dff,dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x, mask):\n        atten_out,_ = self.selfAttention(x,x,x,mask)\n        x = self.norm1(x+ self.dropout(atten_out))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:45:24.368953Z","iopub.execute_input":"2025-11-20T07:45:24.369562Z","iopub.status.idle":"2025-11-20T07:45:24.374501Z","shell.execute_reply.started":"2025-11-20T07:45:24.369542Z","shell.execute_reply":"2025-11-20T07:45:24.373710Z"}},"outputs":[],"execution_count":96},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self,d_model,num_head,dff,dropout=0.1):\n        super(DecoderLayer,self).__init__()\n        self.selfAttention = MultiheadAttention(d_model,num_head,dropout)\n        self.crossAttention = MultiheadAttention(d_model,num_head,dropout)\n        self.feed_forward = PositionwiseFeedForward(d_model,dff,dropout)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self,x,encoder_out,src_mask,tgt_mask):\n        atten_out,_ = self.selfAttention(x,x,x,tgt_mask)\n        x = self.norm1(x+self.dropout(atten_out))\n\n        atten_out,atten_weight = self.crossAttention(x,encoder_out,encoder_out,src_mask)\n        x = self.norm2(x+self.dropout(atten_out))\n        \n        ff_output = self.feed_forward(x)\n        x = self.norm3(x+self.dropout(ff_output))\n\n        return x, atten_weight","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:45:28.998199Z","iopub.execute_input":"2025-11-20T07:45:28.998571Z","iopub.status.idle":"2025-11-20T07:45:29.004564Z","shell.execute_reply.started":"2025-11-20T07:45:28.998548Z","shell.execute_reply":"2025-11-20T07:45:29.003812Z"}},"outputs":[],"execution_count":97},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_len=5000):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_len, d_model)\n        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * \n                           (-math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        pe = pe.unsqueeze(0).transpose(0, 1)\n        \n        self.register_buffer('pe', pe)\n        \n    def forward(self, x):\n        return x + self.pe[:x.size(0), :]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:45:33.510621Z","iopub.execute_input":"2025-11-20T07:45:33.510900Z","iopub.status.idle":"2025-11-20T07:45:33.516586Z","shell.execute_reply.started":"2025-11-20T07:45:33.510877Z","shell.execute_reply":"2025-11-20T07:45:33.515811Z"}},"outputs":[],"execution_count":98},{"cell_type":"code","source":"class Transformer(nn.Module):\n    def __init__(self,src_vocab_size,tgt_vocab_size,d_model = 512,num_head= 8, num_layers = 6, dff= 2048,max_len = 100,dropout =0.1):\n        super(Transformer,self).__init__()\n        self.d_model = d_model\n        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.tgt_embedding = nn.Embedding(tgt_vocab_size,d_model)\n        self.pos_encoding = PositionalEncoding(d_model,max_len)\n\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model,num_head,dff,dropout) for _ in range(num_layers) ])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model,num_head,dff,dropout) for _ in range(num_layers)])\n        self.fc_out = nn.Linear(d_model,tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n\n    def encode(self,src,src_mask):\n        src_embedded = self.dropout(self.pos_encoding(self.src_embedding(src)* math.sqrt(self.d_model)))\n        for layer in self.encoder_layers:\n            src_embedded = layer(src_embedded,src_mask)\n        return src_embedded #Contextualized source representations\n\n    def decode(self,tgt,encoder_output,src_mask,tgt_mask):\n        tgt_embedded = self.dropout(self.pos_encoding(self.tgt_embedding(tgt)*math.sqrt(self.d_model)))\n\n        attention_weights= {}\n        for i, layer in enumerate(self.decoder_layers):\n            tgt_embedded,atten_weights = layer(tgt_embedded,encoder_output,src_mask,tgt_mask)\n            attention_weights[f'decoder_layer_{i+1}'] = atten_weights\n        return tgt_embedded, attention_weights\n\n\n    def forward(self,src,tgt,src_mask,tgt_mask):\n        encoder_output = self.encode(src,src_mask)\n        decoder_output , attention_weights= self.decode(tgt,encoder_output,src_mask,tgt_mask)\n        output = self.fc_out(decoder_output)\n        return output,attention_weights\n            \n        ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:46:34.467920Z","iopub.execute_input":"2025-11-20T07:46:34.469000Z","iopub.status.idle":"2025-11-20T07:46:34.476924Z","shell.execute_reply.started":"2025-11-20T07:46:34.468973Z","shell.execute_reply":"2025-11-20T07:46:34.476162Z"}},"outputs":[],"execution_count":103},{"cell_type":"code","source":"class translationDataset(Dataset):\n    def __init__(self,english_sentences,french_sentences,eng_vocab,fr_vocab,max_len=100):\n        self.english_sentences = english_sentences\n        self.french_sentences = french_sentences\n        self.eng_vocab = eng_vocab\n        self.fr_vocab= fr_vocab\n        self.max_len = max_len\n\n\n    def __len__(self):\n        return len(self.english_sentences)\n\n    def __getitem__(self,idx): # gets one training sample at index idx\n        english_sentence = self.english_sentences.iloc[idx]\n        french_sentence = self.french_sentences.iloc[idx]\n\n        eng_indices = [self.eng_vocab.word2idx['<sos>']] + \\\n                     self.eng_vocab.sentence_to_indices(english_sentence) + \\\n                     [self.eng_vocab.word2idx['<eos>']]\n        \n        fr_indices = [self.fr_vocab.word2idx['<sos>']] + \\\n                    self.fr_vocab.sentence_to_indices(french_sentence) + \\\n                    [self.fr_vocab.word2idx['<eos>']]\n        \n        eng_indices = self.pad_sequence(eng_indices, self.max_len)\n        fr_indices = self.pad_sequence(fr_indices,self.max_len)\n\n        return torch.tensor(eng_indices),torch.tensor(fr_indices)\n\n    def pad_sequence(self,sequence,max_len):\n        if len(sequence) < max_len:\n             sequence = sequence + [self.eng_vocab.word2idx['<pad>']] * (max_len - len(sequence))\n        else :\n            sequence = sequence[:max_len-1]+ [self.eng_vocab.word2idx['<eos']]\n\n        return sequence\n\ntrain_df, val_df = train_test_split(df, test_size = 0.1 , random_state=42) \n\ntrain_dataset = translationDataset(train_df['english'],train_df['french'], eng_vocab,fr_vocab)\nval_dataset = translationDataset( val_df['english'],val_df['french'],eng_vocab,fr_vocab)\n\nbatch_size = 32\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle = True)\ntest_loader = DataLoader(val_dataset,batch_size = batch_size,shuffle = True)\n         ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:46:38.492732Z","iopub.execute_input":"2025-11-20T07:46:38.493309Z","iopub.status.idle":"2025-11-20T07:46:38.529997Z","shell.execute_reply.started":"2025-11-20T07:46:38.493286Z","shell.execute_reply":"2025-11-20T07:46:38.529430Z"}},"outputs":[],"execution_count":104},{"cell_type":"code","source":"def create_masks(src, tgt):\n    src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n    \n    tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n    seq_length = tgt.size(1)\n    nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n    nopeak_mask = nopeak_mask.to(device)\n    tgt_mask = tgt_mask & nopeak_mask\n    \n    return src_mask, tgt_mask\n\nmodel = Transformer(\n    src_vocab_size = eng_vocab.vocab_size,\n    tgt_vocab_size = fr_vocab.vocab_size,\n    d_model = 256,\n    num_head = 8,\n    num_layers = 3 ,\n    dff=512,\n    max_len = 100,\n    dropout = 0.1    \n).to(device)\n\ncriterion = nn.CrossEntropyLoss(ignore_index=0)\noptimizer = optim.Adam(model.parameters(),lr = 0.0001, betas =(0.9,0.98),eps = 1e-9)\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.95)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T07:46:42.730084Z","iopub.execute_input":"2025-11-20T07:46:42.730323Z","iopub.status.idle":"2025-11-20T07:46:42.867380Z","shell.execute_reply.started":"2025-11-20T07:46:42.730306Z","shell.execute_reply":"2025-11-20T07:46:42.866435Z"}},"outputs":[],"execution_count":105},{"cell_type":"code","source":"def train_epoch(model, dataloader, optimizer, criterion):\n    model.train()\n    total_loss = 0\n    \n    for batch_idx, (src, tgt) in enumerate(dataloader):\n        src = src.to(device)\n        tgt = tgt.to(device)\n        \n        # Create masks\n        src_mask, tgt_mask = create_masks(src, tgt[:, :-1])\n        \n        optimizer.zero_grad()\n        \n        # Forward pass\n        output, _ = model(src, tgt[:, :-1], src_mask, tgt_mask)\n        \n        # Calculate loss\n        loss = criterion(output.contiguous().view(-1, output.shape[-1]), \n                        tgt[:, 1:].contiguous().view(-1))\n        \n        # Backward pass\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        optimizer.step()\n        \n        total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\ndef evaluate(model, dataloader, criterion):\n    model.eval()\n    total_loss = 0\n    \n    with torch.no_grad():\n        for src, tgt in dataloader:\n            src = src.to(device)\n            tgt = tgt.to(device)\n            \n            src_mask, tgt_mask = create_masks(src, tgt[:, :-1])\n            \n            output, _ = model(src, tgt[:, :-1], src_mask, tgt_mask)\n            \n            loss = criterion(output.contiguous().view(-1, output.shape[-1]), \n                            tgt[:, 1:].contiguous().view(-1))\n            \n            total_loss += loss.item()\n    \n    return total_loss / len(dataloader)\n\n# Training loop\nnum_epochs = 10\ntrain_losses = []\nval_losses = []\n\nprint(\"Starting Training...\")\nprint(\"=\" * 50)\n\nfor epoch in range(num_epochs):\n    start_time = time.time()\n    \n   \n    train_loss = train_epoch(model, train_loader, optimizer, criterion)\n    \n   \n    val_loss = evaluate(model, test_loader, criterion)\n    \n  \n    scheduler.step()\n    \n  \n    train_losses.append(train_loss)\n    val_losses.append(val_loss)\n    \n    epoch_time = time.time() - start_time\n    \n  \n    print(f'Epoch {epoch+1:02}/{num_epochs} | Time: {epoch_time:.2f}s')\n    print(f'  Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}')\n    \n\nprint(\"Training Completed!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T08:02:39.395350Z","iopub.execute_input":"2025-11-20T08:02:39.395687Z","iopub.status.idle":"2025-11-20T09:06:25.301494Z","shell.execute_reply.started":"2025-11-20T08:02:39.395666Z","shell.execute_reply":"2025-11-20T09:06:25.300684Z"}},"outputs":[{"name":"stdout","text":"Starting Training...\n==================================================\nEpoch 01/10 | Time: 383.10s\n  Train Loss: 2.2306 | Val Loss: 2.0065\nEpoch 02/10 | Time: 382.66s\n  Train Loss: 2.0290 | Val Loss: 1.8700\nEpoch 03/10 | Time: 382.80s\n  Train Loss: 1.8934 | Val Loss: 1.7560\nEpoch 04/10 | Time: 382.76s\n  Train Loss: 1.7940 | Val Loss: 1.6984\nEpoch 05/10 | Time: 382.53s\n  Train Loss: 1.7164 | Val Loss: 1.6368\nEpoch 06/10 | Time: 382.38s\n  Train Loss: 1.6524 | Val Loss: 1.5953\nEpoch 07/10 | Time: 382.69s\n  Train Loss: 1.5999 | Val Loss: 1.5529\nEpoch 08/10 | Time: 382.20s\n  Train Loss: 1.5562 | Val Loss: 1.5238\nEpoch 09/10 | Time: 382.27s\n  Train Loss: 1.5200 | Val Loss: 1.5006\nEpoch 10/10 | Time: 382.51s\n  Train Loss: 1.4901 | Val Loss: 1.4918\nTraining Completed!\n","output_type":"stream"}],"execution_count":109},{"cell_type":"code","source":"def translate_sentence(sentence,model,eng_vocab,fr_vocab,max_len=100):\n    model.eval()\n    sentence = preprocess_text(sentence)\n    tokens = [eng_vocab.word2idx['<sos>']] + eng_vocab.sentence_to_indices(sentence) + [eng_vocab.word2idx['<eos>']]\n    \n    if len(tokens)<max_len:\n        tokens = tokens + [eng_vocab.word2idx['<pad>']] * (max_len - len(tokens))\n    else:\n        tokens = tokens[:max_len-1]+ [eng_vocab.word2idx['<eos>']]\n\n    src = torch.tensor(tokens).unsqueeze(0).to(device)\n    src_mask = (src!=0).unsqueeze(1).unsqueeze(2)\n\n    tgt_tokens = [fr_vocab.word2idx['<sos>']]\n\n    for i in range(max_len):\n        tgt = torch.tensor(tgt_tokens).unsqueeze(0).to(device)\n        tgt_mask = create_masks(src,tgt)[1]\n\n        with torch.no_grad():\n            output,_ = model(src,tgt,src_mask,tgt_mask)\n        pred_token = output.argmax(2)[:,-1].item()\n        tgt_tokens.append(pred_token)\n\n        if pred_token == fr_vocab.word2idx['<eos>']:\n            break\n\n    translated_tokens = [fr_vocab.idx2word[idx] for idx in tgt_tokens if idx not in [fr_vocab.word2idx['<sos>'], fr_vocab.word2idx['<pad>']]]\n    if translated_tokens and translated_tokens[-1] == '<eos>':\n        translated_tokens = translated_tokens[:-1]\n    \n    return ' '.join(translated_tokens)\n\n\ntest_sentence = \"Good morning\"\ntranslation = translate_sentence(test_sentence,model,eng_vocab,fr_vocab)\nprint(f\"English :{test_sentence}\")\nprint(f\"French : {translation}\")\n        \n        \n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-20T09:45:00.378240Z","iopub.execute_input":"2025-11-20T09:45:00.378768Z","iopub.status.idle":"2025-11-20T09:45:00.410006Z","shell.execute_reply.started":"2025-11-20T09:45:00.378742Z","shell.execute_reply":"2025-11-20T09:45:00.409411Z"}},"outputs":[{"name":"stdout","text":"English :Good morning\nFrench : bon matin\n","output_type":"stream"}],"execution_count":120}]}