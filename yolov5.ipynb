{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":5186786,"sourceType":"datasetVersion","datasetId":3015609}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\ndef conv_bn_act(in_ch, out_ch, k=3, s=1, p=1):\n    return nn.Sequential(\n        nn.Conv2d(in_ch, out_ch, k, s, p, bias=False),\n        nn.BatchNorm2d(out_ch),\n        nn.SiLU()\n    )\n\nclass C3(nn.Module):\n    def __init__(self, in_ch, out_ch, n=1):\n        super().__init__()\n        hidden = out_ch // 2\n        self.cv1 = conv_bn_act(in_ch, hidden, k=1, s=1, p=0)\n        self.cv2 = conv_bn_act(in_ch, hidden, k=1, s=1, p=0)\n        self.m = nn.Sequential(\n            *[nn.Sequential(conv_bn_act(hidden, hidden), conv_bn_act(hidden, hidden))\n              for _ in range(n)]\n        )\n        self.cv3 = conv_bn_act(2 * hidden, out_ch, k=1, s=1, p=0)\n\n    def forward(self, x):\n        y1 = self.cv1(x)\n        y2 = self.cv2(x)\n        y1 = self.m(y1)\n        return self.cv3(torch.cat([y1, y2], dim=1))\n\nclass SPPF(nn.Module):\n    def __init__(self, ch, pool_k=5):\n        super().__init__()\n        hidden = ch // 2\n        self.cv1 = conv_bn_act(ch, hidden, k=1, s=1, p=0)\n        self.cv2 = conv_bn_act(hidden * 4, ch, k=1, s=1, p=0)\n        self.pool = nn.MaxPool2d(kernel_size=pool_k, stride=1, padding=pool_k // 2)\n\n    def forward(self, x):\n        x = self.cv1(x)\n        y1 = self.pool(x)\n        y2 = self.pool(y1)\n        y3 = self.pool(y2)\n        return self.cv2(torch.cat([x, y1, y2, y3], dim=1))\n\n\nclass YOLOHead(nn.Module):\n    def __init__(self, in_ch, num_anchors, num_classes):\n        super().__init__()\n        self.conv = conv_bn_act(in_ch, in_ch)\n        self.pred = nn.Conv2d(in_ch, num_anchors * (num_classes + 5), 1, 1, 0)\n        self.num_anchors = num_anchors\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        x = self.conv(x)\n        B, _, H, W = x.shape\n        pred = self.pred(x)\n        pred = pred.view(B, self.num_anchors, self.num_classes + 5, H, W)\n        return pred.permute(0, 1, 3, 4, 2)\n\n\nclass YOLOv5(nn.Module):\n    def __init__(self, nc=80, anchors=None):\n        super().__init__()\n        if anchors is None:\n            anchors = [\n                [[10,13],[16,30],[33,23]],\n                [[30,61],[62,45],[59,119]],\n                [[116,90],[156,198],[373,326]],\n            ]\n\n        self.nc = nc\n        self.anchors = torch.tensor(anchors, dtype=torch.float32)\n\n        # Backbone \n        self.stem = nn.Sequential(conv_bn_act(3, 64, k=6, s=2, p=2), C3(64, 64, n=1))\n        self.down1 = nn.Sequential(conv_bn_act(64, 128, k=3, s=2, p=1), C3(128, 128, n=3))\n        self.down2 = nn.Sequential(conv_bn_act(128, 256, k=3, s=2, p=1), C3(256, 256, n=3))\n        self.down3 = nn.Sequential(conv_bn_act(256, 512, k=3, s=2, p=1), C3(512, 512, n=1))\n        self.sppf = SPPF(512)\n\n        # Neck (PANet-style)\n        self.up1 = conv_bn_act(512, 256, k=1, s=1, p=0)\n        self.pan1 = C3(512, 256, n=1)\n        self.up2 = conv_bn_act(256, 128, k=1, s=1, p=0)\n        self.pan2 = C3(256, 128, n=1)\n\n        # Heads\n        self.head_small = YOLOHead(128, num_anchors=3, num_classes=nc)\n        self.head_medium = YOLOHead(256, num_anchors=3, num_classes=nc)\n        self.head_large = YOLOHead(512, num_anchors=3, num_classes=nc)\n\n    def forward(self, x):\n        # Backbone\n        x = self.stem(x)\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x3 = self.sppf(x3)\n\n        # Neck\n        p5 = x3\n        p5_up = F.interpolate(self.up1(p5), scale_factor=2, mode=\"nearest\")\n        p4 = self.pan1(torch.cat([p5_up, x2], dim=1))\n        p4_up = F.interpolate(self.up2(p4), scale_factor=2, mode=\"nearest\")\n        p3 = self.pan2(torch.cat([p4_up, x1], dim=1))\n\n        # Heads\n        out_small = self.head_small(p3)\n        out_medium = self.head_medium(p4)\n        out_large = self.head_large(p5)\n\n        return [out_small, out_medium, out_large]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T04:16:26.805497Z","iopub.execute_input":"2025-09-04T04:16:26.805658Z","iopub.status.idle":"2025-09-04T04:16:34.382923Z","shell.execute_reply.started":"2025-09-04T04:16:26.805642Z","shell.execute_reply":"2025-09-04T04:16:34.382249Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport os\nimport cv2\nimport numpy as np\nfrom pycocotools.coco import COCO\nimport random\n\nclass COCODetection(Dataset):\n    def __init__(self, root, annFile, img_size=640, transforms=None):\n        self.root = root\n        self.coco = COCO(annFile)\n        self.ids = list(sorted(self.coco.imgs.keys()))\n        self.img_size = img_size\n        self.transforms = transforms\n\n        # Build mapping for COCO category IDs → continuous [0–79]\n        coco_categories = sorted(cat['id'] for cat in self.coco.dataset['categories'])\n        self.cat_id_to_idx = {cat_id: idx for idx, cat_id in enumerate(coco_categories)}\n\n    def __len__(self):\n        return len(self.ids)\n\n    def __getitem__(self, idx):\n        img_id = self.ids[idx]\n        img_info = self.coco.loadImgs(img_id)[0]\n        path = os.path.join(self.root, img_info['file_name'])\n\n        # Load image\n        img = cv2.imread(path)\n        if img is None:\n            raise ValueError(f\"Image at {path} not found\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        h0, w0 = img.shape[:2]\n        scale = self.img_size / max(h0, w0)\n        nh, nw = int(h0 * scale), int(w0 * scale)\n        img_resized = cv2.resize(img, (nw, nh))\n\n        canvas = np.full((self.img_size, self.img_size, 3), 114, dtype=np.uint8)\n        canvas[:nh, :nw] = img_resized\n        img = canvas\n\n        # Load annotations\n        anns = self.coco.loadAnns(self.coco.getAnnIds(imgIds=img_id))\n        boxes = []\n        labels = []\n\n        for a in anns:\n            x, y, w, h = a['bbox']\n            x_c = (x + w / 2) * scale / self.img_size\n            y_c = (y + h / 2) * scale / self.img_size\n            w_n = (w * scale) / self.img_size\n            h_n = (h * scale) / self.img_size\n\n            if w_n <= 0 or h_n <= 0:\n                continue\n\n            boxes.append([x_c, y_c, w_n, h_n])\n            labels.append(self.cat_id_to_idx[a['category_id']])  # Remap ID\n\n        target = {\n            \"boxes\": torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4)),\n            \"labels\": torch.tensor(labels, dtype=torch.long) if labels else torch.zeros((0,), dtype=torch.long)\n        }\n\n        img = img.astype(np.float32) / 255.0\n        img = np.transpose(img, (2, 0, 1))\n\n        return torch.tensor(img, dtype=torch.float32), target\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T04:16:34.384378Z","iopub.execute_input":"2025-09-04T04:16:34.384667Z","iopub.status.idle":"2025-09-04T04:16:34.954111Z","shell.execute_reply.started":"2025-09-04T04:16:34.384650Z","shell.execute_reply":"2025-09-04T04:16:34.953452Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nimport math\n\ndef bbox_iou(box1, box2, eps=1e-7):\n    # boxes in x1y1x2y2\n    inter_x1 = torch.max(box1[...,0], box2[...,0])\n    inter_y1 = torch.max(box1[...,1], box2[...,1])\n    inter_x2 = torch.min(box1[...,2], box2[...,2])\n    inter_y2 = torch.min(box1[...,3], box2[...,3])\n    inter_w = (inter_x2 - inter_x1).clamp(0)\n    inter_h = (inter_y2 - inter_y1).clamp(0)\n    inter = inter_w * inter_h\n    area1 = (box1[...,2]-box1[...,0]).clamp(0) * (box1[...,3]-box1[...,1]).clamp(0)\n    area2 = (box2[...,2]-box2[...,0]).clamp(0) * (box2[...,3]-box2[...,1]).clamp(0)\n    union = area1 + area2 - inter + eps\n    return inter / union\n\ndef xywh2xyxy(x):\n    # x: [N,4] x_c,y_c,w,h normalized\n    x_c, y_c, w, h = x[...,0], x[...,1], x[...,2], x[...,3]\n    x1 = x_c - w/2\n    y1 = y_c - h/2\n    x2 = x_c + w/2\n    y2 = y_c + h/2\n    return torch.stack([x1,y1,x2,y2], dim=-1)\n\ndef compute_loss(preds, targets, anchors, strides, device, num_classes=80):\n    \n    #preds: list of prediction tensors [B, A, H, W, 5+nc] for 3 scales\n    #targets: list of targets per image (boxes normalized to 0-1 and labels)\n    #anchors: tensor shape (3,3,2) or list of 3 lists of 3 pairs\n    #strides: list of strides per scale (tuple)\n    #returns total_loss, dict breakdown\n   \n    bce = torch.nn.BCEWithLogitsLoss(reduction='mean')\n    mse = torch.nn.MSELoss(reduction='mean')\n    device = device\n    loss_obj = torch.tensor(0., device=device)\n    loss_cls = torch.tensor(0., device=device)\n    loss_box = torch.tensor(0., device=device)\n    # naive matching: for every target box, find best scale based on size and assign to its grid cell & best anchor by IoU\n    B = preds[0].shape[0]\n    for b in range(B):\n        t = targets[b]\n        if t['boxes'].numel() == 0:\n            # no objects: encourage objectness to 0 across all preds\n            for p in preds:\n                obj_pred = p[b,...,4]\n                loss_obj = loss_obj + bce(obj_pred, torch.zeros_like(obj_pred))\n            continue\n        boxes = t['boxes'].to(device)  # normalized xywh\n        labels = t['labels'].to(device)\n        # for each box, compute which scale to place (based on area) and compute losses\n        for i_box in range(boxes.shape[0]):\n            box = boxes[i_box:i_box+1]  # 1,4\n            lab = labels[i_box:i_box+1]\n            # choose scale by width (very rough heuristic)\n            w = box[0,2]\n            if w < 0.06:\n                scale_idx = 0\n            elif w < 0.2:\n                scale_idx = 1\n            else:\n                scale_idx = 2\n            p = preds[scale_idx][b]  # [A, H, W, 5+nc]\n            A, H, W, _ = p.shape\n            # map center to grid\n            gx = box[0,0] * W\n            gy = box[0,1] * H\n            gi = int(gx.clamp(0, W-1).item())\n            gj = int(gy.clamp(0, H-1).item())\n            # for anchor matching pick best anchor by comparing anchor box ratios vs target\n            anchor_set = anchors[scale_idx].to(device) / strides[scale_idx]\n            # compute IoU between scaled anchors (centered) and target w,h\n            tw = box[0,2] * W\n            th = box[0,3] * H\n            anchor_wh = anchor_set\n            # convert to xyxy\n            a_box = torch.zeros((anchor_wh.shape[0],4), device=device)\n            a_box[:,0:2] = torch.stack([torch.zeros_like(anchor_wh[:,0]), torch.zeros_like(anchor_wh[:,1])], dim=1)\n            a_box[:,2] = anchor_wh[:,0]\n            a_box[:,3] = anchor_wh[:,1]\n            t_box = torch.tensor([0,0,tw,th], device=device).unsqueeze(0)\n            # intersection over anchors: use min(w,h) heuristic\n            inter = torch.min(a_box[:,2], t_box[:,2]) * torch.min(a_box[:,3], t_box[:,3])\n            area_a = a_box[:,2]*a_box[:,3]\n            area_t = tw*th\n            ious = inter / (area_a + area_t - inter + 1e-9)\n            best_anchor = int(torch.argmax(ious).item())\n            # predictions at anchor/grid\n            pred = p[best_anchor, gj, gi]  # (5+nc)\n            # decode pred: tx,ty,tw,th\n            # For simplicity assume pred[...,0:2] are offsets (sigmoid), 2:4 are log-space, 4 is obj, 5: are class logits\n            px = (torch.sigmoid(pred[0]) + gx - gi) / W  # normalized\n            py = (torch.sigmoid(pred[1]) + gy - gj) / H\n            pw = (torch.exp(pred[2]) * anchor_wh[best_anchor,0]) / (W)\n            ph = (torch.exp(pred[3]) * anchor_wh[best_anchor,1]) / (H)\n            pred_box = torch.stack([px, py, pw, ph], dim=0).unsqueeze(0)\n            # box loss: use IoU between predicted box and target\n            iou = bbox_iou(xywh2xyxy(pred_box), xywh2xyxy(box.to(device)))\n            loss_box = loss_box + (1.0 - iou).squeeze()\n            # objectness: target 1 for this cell/anchor, 0 elsewhere\n            obj_pred = pred[4]\n            loss_obj = loss_obj + bce(obj_pred.unsqueeze(0), torch.ones((1,), device=device))\n            # classification\n            cls_pred = pred[5:]\n            target_cls = torch.zeros_like(cls_pred)\n            target_cls[lab] = 1.0\n            loss_cls = loss_cls + bce(cls_pred.unsqueeze(0), target_cls.unsqueeze(0))\n    # normalize by batch size\n    denom = max(1, B)\n    return (loss_box/denom, loss_obj/denom, loss_cls/denom), {\"box\":loss_box.item()/denom, \"obj\":loss_obj.item()/denom, \"cls\":loss_cls.item()/denom}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T04:16:34.957228Z","iopub.execute_input":"2025-09-04T04:16:34.957498Z","iopub.status.idle":"2025-09-04T04:16:34.974316Z","shell.execute_reply.started":"2025-09-04T04:16:34.957473Z","shell.execute_reply":"2025-09-04T04:16:34.973545Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nimport os\n\nclass Config:\n    img_root = \"/kaggle/input/2017-2017/train2017/train2017\"\n    ann = \"/kaggle/input/2017-2017/annotations_trainval2017/annotations/instances_train2017.json\"\n    img_size = 640\n    batch = 8\n    epochs = 10\n    lr = 1e-3\n    num_classes = 80\n    save_dir = \"./weights\"\n\ncfg = Config()\n\n\ndef train(cfg):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    print(f\"Training on: {device}\")\n\n    # Initialize model\n    model = YOLOv5(nc=cfg.num_classes).to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=cfg.lr, weight_decay=5e-4)\n\n    anchors = torch.tensor([\n        [[10,13],[16,30],[33,23]],\n        [[30,61],[62,45],[59,119]],\n        [[116,90],[156,198],[373,326]]\n    ], dtype=torch.float32)\n    strides = [8, 16, 32]\n\n    # Dataset and DataLoader\n    train_ds = COCODetection(cfg.img_root, cfg.ann, img_size=cfg.img_size)\n    loader = DataLoader(train_ds, batch_size=cfg.batch, shuffle=True, num_workers=4, collate_fn=collate_fn)\n\n    model.train()\n    for epoch in range(1, cfg.epochs + 1):\n        pbar = tqdm(loader, desc=f\"Epoch {epoch}\")\n        epoch_loss = 0.0\n\n        for imgs, targets in pbar:\n            imgs = imgs.to(device)\n\n            # Forward\n            preds = model(imgs)\n\n            # Loss computation\n            loss_vals, breakdown = compute_loss(preds, targets, anchors, strides, device, num_classes=cfg.num_classes)\n            loss = sum(loss_vals)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n            pbar.set_postfix(loss=epoch_loss / (pbar.n + 1),\n                             box=breakdown['box'],\n                             obj=breakdown['obj'],\n                             cls=breakdown['cls'])\n\n        # Save checkpoints\n        os.makedirs(cfg.save_dir, exist_ok=True)\n        torch.save({'model': model.state_dict(),\n                    'optimizer': optimizer.state_dict()},\n                   os.path.join(cfg.save_dir, f\"ckpt_epoch{epoch}.pt\"))\n\ndef collate_fn(batch):\n    imgs = torch.stack([i[0] for i in batch], 0)\n    targets = [i[1] for i in batch]\n    return imgs, targets\n\n\nif __name__ == \"__main__\":\n    train(cfg)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-04T04:16:34.975036Z","iopub.execute_input":"2025-09-04T04:16:34.975231Z","execution_failed":"2025-09-04T08:23:00.414Z"}},"outputs":[{"name":"stdout","text":"Training on: cuda\nloading annotations into memory...\nDone (t=21.71s)\ncreating index...\nindex created!\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1:  99%|█████████▉| 14637/14786 [3:56:02<02:35,  1.04s/it, box=11.6, cls=0.583, loss=7.55, obj=0.116]     ","output_type":"stream"}],"execution_count":null}]}