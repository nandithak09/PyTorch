{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfcba2db-c4c8-4f5c-a73f-4ea42200bce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# tuple = (out_channels, kernel_size, stride)\n",
    "# List = [\"B\",num_repeats]\n",
    "# S= Scale prediction\n",
    "# U= upsampling and concatenating with previous layer\n",
    "\n",
    "config = [\n",
    "    (32, 3,1),\n",
    "    (64, 3, 2),\n",
    "    [\"B\" , 1],\n",
    "    (128, 3 ,2),\n",
    "    [\"B\", 2],\n",
    "    (256, 3, 2),\n",
    "    [\"B\" , 8],\n",
    "    # first route from the end of the previous block\n",
    "    (512, 3, 2),\n",
    "    [\"B\", 8],\n",
    "      # second route from the end of the previous block\n",
    "    (1024, 3, 2),\n",
    "    [\"B\", 4], # To this point it is darknet-53\n",
    "    (512, 1, 1),\n",
    "    (1024, 3, 1),\n",
    "    \"S\",\n",
    "    (256, 1, 1),\n",
    "    \"U\",\n",
    "    (256, 1, 1),\n",
    "    (512, 3, 1),\n",
    "    \"S\",\n",
    "    (128, 1, 1),\n",
    "    \"U\",\n",
    "    (128, 1, 1),\n",
    "    (256, 3, 1),\n",
    "    \"S\",\n",
    "]\n",
    "\n",
    "class CNNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.1)\n",
    "        self.use_bn_act = bn_act\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.use_bn_act:\n",
    "            return self.leaky(self.bn(self.conv(x)))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels, use_residual=True, num_repeats=1):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for repeat in range(num_repeats):\n",
    "            self.layers += [\n",
    "                nn.Sequential(\n",
    "                    CNNBlock(channels, channels // 2, kernel_size=1),\n",
    "                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n",
    "                )\n",
    "            ]\n",
    "\n",
    "        self.use_residual = use_residual\n",
    "        self.num_repeats = num_repeats\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            if self.use_residual:\n",
    "                x = x + layer(x)\n",
    "            else:\n",
    "                x = layer(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class ScalePrediction(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        self.pred = nn.Sequential(\n",
    "            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n",
    "            CNNBlock(\n",
    "                2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1\n",
    "            ),\n",
    "        )\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return (\n",
    "            self.pred(x)\n",
    "            .reshape(x.shape[0], 3, self.num_classes + 5, x.shape[2], x.shape[3])\n",
    "            .permute(0, 1, 3, 4, 2)\n",
    "        ) # (B , 3, num_classes+5, W, H) => (B, 3, H, W, num_classes+5)\n",
    "\n",
    "\n",
    "class YOLOv3(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=20):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.in_channels = in_channels\n",
    "        self.layers = self._create_conv_layers()\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []  # for each scale\n",
    "        route_connections = []\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, ScalePrediction):\n",
    "                outputs.append(layer(x))\n",
    "                continue\n",
    "\n",
    "            x = layer(x)\n",
    "\n",
    "            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n",
    "                route_connections.append(x)\n",
    "\n",
    "            elif isinstance(layer, nn.Upsample):\n",
    "                x = torch.cat([x, route_connections[-1]], dim=1)\n",
    "                route_connections.pop()\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def _create_conv_layers(self):\n",
    "        layers = nn.ModuleList()\n",
    "        in_channels = self.in_channels\n",
    "\n",
    "        for module in config:\n",
    "            if isinstance(module, tuple):\n",
    "                out_channels, kernel_size, stride = module\n",
    "                layers.append(\n",
    "                    CNNBlock(\n",
    "                        in_channels,\n",
    "                        out_channels,\n",
    "                        kernel_size=kernel_size,\n",
    "                        stride=stride,\n",
    "                        padding=1 if kernel_size == 3 else 0,\n",
    "                    )\n",
    "                )\n",
    "                in_channels = out_channels\n",
    "\n",
    "            elif isinstance(module, list):\n",
    "                num_repeats = module[1]\n",
    "                layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n",
    "\n",
    "            elif isinstance(module, str):\n",
    "                if module == \"S\":\n",
    "                    layers += [\n",
    "                        ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n",
    "                        CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n",
    "                        ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n",
    "                    ]\n",
    "                    in_channels = in_channels // 2\n",
    "\n",
    "                elif module == \"U\":\n",
    "                    layers.append(nn.Upsample(scale_factor=2),)\n",
    "                    in_channels = in_channels * 3\n",
    "\n",
    "        return layers                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6e1f220-1e6c-4c5c-b1ab-ab2d642cdca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image, ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa52b15-ce61-4d60-8b13-94560211d1c7",
   "metadata": {},
   "source": [
    "# Intersection over Union (IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47acce4c-bf25-4043-a84c-a8b9039cd698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function to calculate Intersection over Union (IoU)\n",
    "def iou(box1, box2, is_pred=True):\n",
    "    if is_pred:\n",
    "        # IoU score for prediction and label\n",
    "        # box1 (prediction) and box2 (label) are both in [x, y, width, height] format\n",
    "        \n",
    "        # Box coordinates of prediction\n",
    "        b1_x1 = box1[..., 0:1] - box1[..., 2:3] / 2\n",
    "        b1_y1 = box1[..., 1:2] - box1[..., 3:4] / 2\n",
    "        b1_x2 = box1[..., 0:1] + box1[..., 2:3] / 2\n",
    "        b1_y2 = box1[..., 1:2] + box1[..., 3:4] / 2\n",
    "\n",
    "        # Box coordinates of ground truth\n",
    "        b2_x1 = box2[..., 0:1] - box2[..., 2:3] / 2\n",
    "        b2_y1 = box2[..., 1:2] - box2[..., 3:4] / 2\n",
    "        b2_x2 = box2[..., 0:1] + box2[..., 2:3] / 2\n",
    "        b2_y2 = box2[..., 1:2] + box2[..., 3:4] / 2\n",
    "\n",
    "        # Get the coordinates of the intersection rectangle\n",
    "        x1 = torch.max(b1_x1, b2_x1)\n",
    "        y1 = torch.max(b1_y1, b2_y1)\n",
    "        x2 = torch.min(b1_x2, b2_x2)\n",
    "        y2 = torch.min(b1_y2, b2_y2)\n",
    "        # Make sure the intersection is at least 0\n",
    "        intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
    "\n",
    "        # Calculate the union area\n",
    "        box1_area = abs((b1_x2 - b1_x1) * (b1_y2 - b1_y1))\n",
    "        box2_area = abs((b2_x2 - b2_x1) * (b2_y2 - b2_y1))\n",
    "        union = box1_area + box2_area - intersection\n",
    "\n",
    "        # Calculate the IoU score\n",
    "        epsilon = 1e-6\n",
    "        iou_score = intersection / (union + epsilon)\n",
    "\n",
    "        # Return IoU score\n",
    "        return iou_score\n",
    "    \n",
    "    else:\n",
    "        # IoU score based on width and height of bounding boxes\n",
    "        \n",
    "        # Calculate intersection area\n",
    "        intersection_area = torch.min(box1[..., 0], box2[..., 0]) * \\\n",
    "                            torch.min(box1[..., 1], box2[..., 1])\n",
    "\n",
    "        # Calculate union area\n",
    "        box1_area = box1[..., 0] * box1[..., 1]\n",
    "        box2_area = box2[..., 0] * box2[..., 1]\n",
    "        union_area = box1_area + box2_area - intersection_area\n",
    "\n",
    "        # Calculate IoU score\n",
    "        iou_score = intersection_area / union_area\n",
    "\n",
    "        # Return IoU score\n",
    "        return iou_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196a5f7d-6083-413a-97d1-b65d5e92c049",
   "metadata": {},
   "source": [
    "# Non-maximum suppression function to remove overlapping bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da441a17-c049-4732-a9ac-764f6cb1000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-maximum suppression function to remove overlapping bounding boxes\n",
    "def nms(bboxes, iou_threshold, threshold):\n",
    "    # Filter out bounding boxes with confidence below the threshold.\n",
    "    bboxes = [box for box in bboxes if box[1] > threshold]\n",
    "\n",
    "    # Sort the bounding boxes by confidence in descending order.\n",
    "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Initialize the list of bounding boxes after non-maximum suppression.\n",
    "    bboxes_nms = []\n",
    "\n",
    "    while bboxes:\n",
    "        # Get the first bounding box.\n",
    "        first_box = bboxes.pop(0)\n",
    "\n",
    "        # Iterate over the remaining bounding boxes.\n",
    "        for box in bboxes:\n",
    "        # If the bounding boxes do not overlap or if the first bounding box has\n",
    "        # a higher confidence, then add the second bounding box to the list of\n",
    "        # bounding boxes after non-maximum suppression.\n",
    "            if box[0] != first_box[0] or iou(\n",
    "                torch.tensor(first_box[2:]),\n",
    "                torch.tensor(box[2:]),\n",
    "            ) < iou_threshold:\n",
    "                # Check if box is not in bboxes_nms\n",
    "                if box not in bboxes_nms:\n",
    "                    # Add box to bboxes_nms\n",
    "                    bboxes_nms.append(box)\n",
    "\n",
    "    # Return bounding boxes after non-maximum suppression.\n",
    "    return bboxes_nms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91c7fd74-7f96-403f-8f8d-09d94f0eb153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to save checkpoint\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"==> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "858732ff-03c2-45a3-a296-f0a4cd66a0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load checkpoint\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"==> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=device)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12ae9cdc-bbb5-42ce-8fc8-a24e91617922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants\n",
    "\n",
    "# Device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load and save model variable\n",
    "load_model = False\n",
    "save_model = True\n",
    "\n",
    "# model checkpoint file name\n",
    "checkpoint_file = \"checkpoint.pth.tar\"\n",
    "\n",
    "# Anchor boxes for each feature map scaled between 0 and 1\n",
    "# 3 feature maps at 3 different scales based on YOLOv3 paper\n",
    "ANCHORS = [\n",
    "    [(0.28, 0.22), (0.38, 0.48), (0.9, 0.78)],\n",
    "    [(0.07, 0.15), (0.15, 0.11), (0.14, 0.29)],\n",
    "    [(0.02, 0.03), (0.04, 0.07), (0.08, 0.06)],\n",
    "]\n",
    "\n",
    "# Batch size for training\n",
    "batch_size = 32\n",
    "\n",
    "# Learning rate for training\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Number of epochs for training\n",
    "epochs = 20\n",
    "\n",
    "# Image size\n",
    "image_size = 416\n",
    "\n",
    "# Grid cell sizes\n",
    "s = [image_size // 32, image_size // 16, image_size // 8]\n",
    "\n",
    "# Class labels\n",
    "class_labels = [\n",
    "    \"aeroplane\", \"bicycle\", \"bird\", \"boat\", \"bottle\", \"bus\", \"car\", \"cat\",\n",
    "    \"chair\", \"cow\", \"diningtable\", \"dog\", \"horse\", \"motorbike\", \"person\",\n",
    "    \"pottedplant\", \"sheep\", \"sofa\", \"train\", \"tvmonitor\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbc4928-ec6f-4802-8616-1aec18dbb75a",
   "metadata": {},
   "source": [
    "# DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec1ed5b8-6969-4bc9-b02b-4f668f39c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset class to load the images and labels from the folder\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self, csv_file, image_dir, label_dir, anchors, \n",
    "        image_size=416, grid_sizes=[13, 26, 52],\n",
    "        num_classes=20, transform=None\n",
    "    ):\n",
    "        # Read the csv file with image names and labels\n",
    "        self.label_list = pd.read_csv(csv_file)\n",
    "        # Image and label directories\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        # Image size\n",
    "        self.image_size = image_size\n",
    "        # Transformations\n",
    "        self.transform = transform\n",
    "        # Grid sizes for each scale\n",
    "        self.grid_sizes = grid_sizes\n",
    "        # Anchor boxes\n",
    "        self.anchors = torch.tensor(\n",
    "            anchors[0] + anchors[1] + anchors[2])\n",
    "        # Number of anchor boxes \n",
    "        self.num_anchors = self.anchors.shape[0]\n",
    "        # Number of anchor boxes per scale\n",
    "        self.num_anchors_per_scale = self.num_anchors // 3\n",
    "        # Number of classes\n",
    "        self.num_classes = num_classes\n",
    "        # Ignore IoU threshold\n",
    "        self.ignore_iou_thresh = 0.5\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Getting the label path\n",
    "        label_path = os.path.join(self.label_dir, self.label_list.iloc[idx, 1])\n",
    "        # We are applying roll to move class label to the last column\n",
    "        # 5 columns: x, y, width, height, class_label\n",
    "        bboxes = np.roll(np.loadtxt(fname=label_path,\n",
    "                         delimiter=\" \", ndmin=2), 4, axis=1).tolist()\n",
    "        \n",
    "        # Getting the image path\n",
    "        img_path = os.path.join(self.image_dir, self.label_list.iloc[idx, 0])\n",
    "        image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "\n",
    "        # Albumentations augmentations\n",
    "        if self.transform:\n",
    "            augs = self.transform(image=image, bboxes=bboxes)\n",
    "            image = augs[\"image\"]\n",
    "            bboxes = augs[\"bboxes\"]\n",
    "\n",
    "        # Below assumes 3 scale predictions (as paper) and same num of anchors per scale\n",
    "        # target : [probabilities, x, y, width, height, class_label]\n",
    "        targets = [torch.zeros((self.num_anchors_per_scale, s, s, 6))\n",
    "                   for s in self.grid_sizes]\n",
    "        \n",
    "        # Identify anchor box and cell for each bounding box\n",
    "        for box in bboxes:\n",
    "            # Calculate iou of bounding box with anchor boxes\n",
    "            iou_anchors = iou(torch.tensor(box[2:4]), \n",
    "                              self.anchors, \n",
    "                              is_pred=False)\n",
    "            # Selecting the best anchor box\n",
    "            anchor_indices = iou_anchors.argsort(descending=True, dim=0)\n",
    "            x, y, width, height, class_label = box\n",
    "\n",
    "            # At each scale, assigning the bounding box to the \n",
    "            # best matching anchor box\n",
    "            has_anchor = [False] * 3\n",
    "            for anchor_idx in anchor_indices:\n",
    "                scale_idx = anchor_idx // self.num_anchors_per_scale\n",
    "                anchor_on_scale = anchor_idx % self.num_anchors_per_scale\n",
    "                \n",
    "                # Identifying the grid size for the scale\n",
    "                s = self.grid_sizes[scale_idx]\n",
    "                \n",
    "                # Identifying the cell to which the bounding box belongs\n",
    "                i, j = int(s * y), int(s * x)\n",
    "                anchor_taken = targets[scale_idx][anchor_on_scale, i, j, 0]\n",
    "                \n",
    "                # Check if the anchor box is already assigned\n",
    "                if not anchor_taken and not has_anchor[scale_idx]:\n",
    "\n",
    "                    # Set the probability to 1\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = 1\n",
    "\n",
    "                    # Calculating the center of the bounding box relative\n",
    "                    # to the cell\n",
    "                    x_cell, y_cell = s * x - j, s * y - i \n",
    "\n",
    "                    # Calculating the width and height of the bounding box \n",
    "                    # relative to the cell\n",
    "                    width_cell, height_cell = (width * s, height * s)\n",
    "\n",
    "                    # Idnetify the box coordinates\n",
    "                    box_coordinates = torch.tensor(\n",
    "                                        [x_cell, y_cell, width_cell, \n",
    "                                         height_cell]\n",
    "                                    )\n",
    "\n",
    "                    # Assigning the box coordinates to the target\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 1:5] = box_coordinates\n",
    "\n",
    "                    # Assigning the class label to the target\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 5] = int(class_label)\n",
    "\n",
    "                    # Set the anchor box as assigned for the scale\n",
    "                    has_anchor[scale_idx] = True\n",
    "\n",
    "                # If the anchor box is already assigned, check if the \n",
    "                # IoU is greater than the threshold\n",
    "                elif not anchor_taken and iou_anchors[anchor_idx] > self.ignore_iou_thresh:\n",
    "                    # Set the probability to -1 to ignore the anchor box\n",
    "                    targets[scale_idx][anchor_on_scale, i, j, 0] = -1\n",
    "\n",
    "        # Return the image and the target\n",
    "        return image, tuple(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26ac7fcd-df02-4426-80e8-a44fcf31a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transformations:\n",
    "\n",
    "# Transform for training\n",
    "train_transform = A.Compose(\n",
    "    [\n",
    "        # Rescale an image so that maximum side is equal to image_size\n",
    "        A.LongestMaxSize(max_size=image_size),\n",
    "        # Pad remaining areas with zeros\n",
    "        A.PadIfNeeded(\n",
    "            min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        # Random color jittering\n",
    "        A.ColorJitter(\n",
    "            brightness=0.5, contrast=0.5,\n",
    "            saturation=0.5, hue=0.5, p=0.5\n",
    "        ),\n",
    "        # Flip the image horizontally\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        # Normalize the image\n",
    "        A.Normalize(\n",
    "            mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255\n",
    "        ),\n",
    "        # Convert the image to PyTorch tensor\n",
    "        ToTensorV2()\n",
    "    ], \n",
    "    # Augmentation for bounding boxes\n",
    "    bbox_params=A.BboxParams(\n",
    "                    format=\"yolo\", \n",
    "                    min_visibility=0.4, \n",
    "                    label_fields=[]\n",
    "                )\n",
    ")\n",
    "\n",
    "# Transform for testing\n",
    "test_transform = A.Compose(\n",
    "    [\n",
    "        # Rescale an image so that maximum side is equal to image_size\n",
    "        A.LongestMaxSize(max_size=image_size),\n",
    "        # Pad remaining areas with zeros\n",
    "        A.PadIfNeeded(\n",
    "            min_height=image_size, min_width=image_size, border_mode=cv2.BORDER_CONSTANT\n",
    "        ),\n",
    "        # Normalize the image\n",
    "        A.Normalize(\n",
    "            mean=[0, 0, 0], std=[1, 1, 1], max_pixel_value=255\n",
    "        ),\n",
    "        # Convert the image to PyTorch tensor\n",
    "        ToTensorV2()\n",
    "    ],\n",
    "    # Augmentation for bounding boxes \n",
    "    bbox_params=A.BboxParams(\n",
    "                    format=\"yolo\", \n",
    "                    min_visibility=0.4, \n",
    "                    label_fields=[]\n",
    "                )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556f4dcb-220b-4fea-8287-f6538825c2b0",
   "metadata": {},
   "source": [
    "# Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78fc38cf-317a-478e-8bc9-3a5c09f98b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining YOLO loss class\n",
    "class YOLOLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.cross_entropy = nn.CrossEntropyLoss()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, pred, target, anchors):\n",
    "        # Identifying which cells in target have objects \n",
    "        # and which have no objects\n",
    "        obj = target[..., 0] == 1\n",
    "        no_obj = target[..., 0] == 0\n",
    "\n",
    "        # Calculating No object loss\n",
    "        no_object_loss = self.bce(\n",
    "            (pred[..., 0:1][no_obj]), (target[..., 0:1][no_obj]),\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Reshaping anchors to match predictions\n",
    "        anchors = anchors.reshape(1, 3, 1, 1, 2)\n",
    "        # Box prediction confidence\n",
    "        box_preds = torch.cat([self.sigmoid(pred[..., 1:3]),\n",
    "                               torch.exp(pred[..., 3:5]) * anchors\n",
    "                            ],dim=-1)\n",
    "        # Calculating intersection over union for prediction and target\n",
    "        ious = iou(box_preds[obj], target[..., 1:5][obj]).detach()\n",
    "        # Calculating Object loss\n",
    "        object_loss = self.mse(self.sigmoid(pred[..., 0:1][obj]),\n",
    "                               ious * target[..., 0:1][obj])\n",
    "\n",
    "        \n",
    "        # Predicted box coordinates\n",
    "        pred[..., 1:3] = self.sigmoid(pred[..., 1:3])\n",
    "        # Target box coordinates\n",
    "        target[..., 3:5] = torch.log(1e-6 + target[..., 3:5] / anchors)\n",
    "        # Calculating box coordinate loss\n",
    "        box_loss = self.mse(pred[..., 1:5][obj],\n",
    "                            target[..., 1:5][obj])\n",
    "\n",
    "        \n",
    "        # Claculating class loss\n",
    "        class_loss = self.cross_entropy((pred[..., 5:][obj]),\n",
    "                                   target[..., 5][obj].long())\n",
    "\n",
    "        # Total loss\n",
    "        return (\n",
    "            box_loss\n",
    "            + object_loss\n",
    "            + no_object_loss\n",
    "            + class_loss\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b5e154e-5012-4dcf-b72a-5026bbab17d6",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dee821e-8afd-4188-b3c9-1b1563cea220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [12:30<00:00, 187.68s/it, loss=18.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [16:18<00:00, 244.71s/it, loss=18.2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [04:29<00:00, 67.41s/it, loss=17.8]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Saving checkpoint\n",
      "Epoch: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Set device to CPU\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# Define the train function to train the model\n",
    "def training_loop(loader, model, optimizer, loss_fn, scaled_anchors):\n",
    "    # Creating a progress bar\n",
    "    progress_bar = tqdm(loader, leave=True)\n",
    "\n",
    "    # Initializing a list to store the losses\n",
    "    losses = []\n",
    "\n",
    "    # Iterating over the training data\n",
    "    for _, (x, y) in enumerate(progress_bar):\n",
    "        x = x.to(device)\n",
    "        y0, y1, y2 = (\n",
    "            y[0].to(device),\n",
    "            y[1].to(device),\n",
    "            y[2].to(device),\n",
    "        )\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(x)\n",
    "\n",
    "        loss = YOLOLoss()\n",
    "\n",
    "        # Reset gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "\n",
    "        # Optimization step\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "# Creating the model from YOLOv3 class\n",
    "model = YOLOv3().to(device)\n",
    "\n",
    "# Defining the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Defining the train dataset\n",
    "train_dataset = Dataset(\n",
    "    csv_file=\"vocData/100examples.csv\",\n",
    "    image_dir=\"vocData/images/\",\n",
    "    label_dir=\"vocData/labels/\",\n",
    "    anchors=ANCHORS,\n",
    "    transform=train_transform\n",
    ")\n",
    "\n",
    "# Defining the train data loader\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=0,   \n",
    "    shuffle=True,\n",
    "    pin_memory=False\n",
    ")\n",
    "\n",
    "# Scaling the anchors\n",
    "scaled_anchors = (\n",
    "    torch.tensor(ANCHORS) * \n",
    "    torch.tensor(s).unsqueeze(1).unsqueeze(1).repeat(1, 3, 2)\n",
    ").to(device)\n",
    "\n",
    "# Training the model\n",
    "for e in range(1, epochs + 1):\n",
    "    print(\"Epoch:\", e)\n",
    "    training_loop(train_loader, model, optimizer, loss_fn, scaled_anchors)\n",
    "\n",
    "    # Saving the model\n",
    "    if save_model:\n",
    "        save_checkpoint(model, optimizer, filename=f\"checkpoint.pth.tar\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f040ff-d499-46be-b8f4-3d835c54ee44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
