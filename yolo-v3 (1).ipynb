{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3866417,"sourceType":"datasetVersion","datasetId":843852}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport math\nimport random\nfrom pathlib import Path\nfrom collections import defaultdict\n\nimport cv2\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport torchvision.ops as ops\n\nconfig = [\n    (32, 3,1),\n    (64, 3, 2),\n    [\"B\" , 1],\n    (128, 3 ,2),\n    [\"B\", 2],\n    (256, 3, 2),\n    [\"B\" , 8],\n    (512, 3, 2),\n    [\"B\", 8],\n    (1024, 3, 2),\n    [\"B\", 4],\n    (512, 1, 1),\n    (1024, 3, 1),\n    \"S\",\n    (256, 1, 1),\n    \"U\",\n    (256, 1, 1),\n    (512, 3, 1),\n    \"S\",\n    (128, 1, 1),\n    \"U\",\n    (128, 1, 1),\n    (256, 3, 1),\n    \"S\",\n]\n\nclass CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, bn_act=True, **kwargs):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, bias=not bn_act, **kwargs)\n        self.bn = nn.BatchNorm2d(out_channels) if bn_act else nn.Identity()\n        self.leaky = nn.LeakyReLU(0.1)\n        self.use_bn_act = bn_act\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.use_bn_act:\n            x = self.bn(x)\n            x = self.leaky(x)\n        return x\n\nclass ResidualBlock(nn.Module):\n    def __init__(self, channels, use_residual=True, num_repeats=1):\n        super().__init__()\n        self.layers = nn.ModuleList()\n        for _ in range(num_repeats):\n            self.layers.append(\n                nn.Sequential(\n                    CNNBlock(channels, channels // 2, kernel_size=1),\n                    CNNBlock(channels // 2, channels, kernel_size=3, padding=1),\n                )\n            )\n        self.use_residual = use_residual\n        self.num_repeats = num_repeats\n\n    def forward(self, x):\n        for layer in self.layers:\n            if self.use_residual:\n                x = x + layer(x)\n            else:\n                x = layer(x)\n        return x\n\nclass ScalePrediction(nn.Module):\n    def __init__(self, in_channels, num_classes):\n        super().__init__()\n        self.pred = nn.Sequential(\n            CNNBlock(in_channels, 2 * in_channels, kernel_size=3, padding=1),\n            CNNBlock(2 * in_channels, (num_classes + 5) * 3, bn_act=False, kernel_size=1),\n        )\n        self.num_classes = num_classes\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        return (\n            self.pred(x)\n            .reshape(B, 3, self.num_classes + 5, H, W)\n            .permute(0, 1, 3, 4, 2)\n        )\n\nclass YOLOv3(nn.Module):\n    def __init__(self, in_channels=3, num_classes=1):\n        super().__init__()\n        self.num_classes = num_classes\n        self.in_channels = in_channels\n        self.layers = self._create_conv_layers()\n\n    def forward(self, x):\n        outputs = []\n        route_connections = []\n\n        for layer in self.layers:\n            if isinstance(layer, ScalePrediction):\n                outputs.append(layer(x))\n                continue\n\n            x = layer(x)\n\n            # store route from last residual block with 8 repeats\n            if isinstance(layer, ResidualBlock) and layer.num_repeats == 8:\n                route_connections.append(x)\n\n            elif isinstance(layer, nn.Upsample):\n                x = torch.cat([x, route_connections[-1]], dim=1)\n                route_connections.pop()\n        return outputs\n\n    def _create_conv_layers(self):\n            layers = nn.ModuleList()\n            in_channels = self.in_channels\n    \n            for module in config:\n                if isinstance(module, tuple):\n                    out_channels, kernel_size, stride = module\n                    layers.append(\n                        CNNBlock(\n                            in_channels,\n                            out_channels,\n                            kernel_size=kernel_size,\n                            stride=stride,\n                            padding=1 if kernel_size == 3 else 0,\n                        )\n                    )\n                    in_channels = out_channels\n    \n                elif isinstance(module, list):\n                    num_repeats = module[1]\n                    layers.append(ResidualBlock(in_channels, num_repeats=num_repeats,))\n                \n                elif isinstance(module, str):\n                    if module == \"S\":\n                        layers += [\n                            ResidualBlock(in_channels, use_residual=False, num_repeats=1),\n                            CNNBlock(in_channels, in_channels // 2, kernel_size=1),\n                            ScalePrediction(in_channels // 2, num_classes=self.num_classes),\n                        ]\n                        in_channels = in_channels // 2\n        \n                    elif module == \"U\":\n                        layers.append(nn.Upsample(scale_factor=2),)\n                        in_channels = in_channels * 3\n        \n            return layers\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:09:27.201818Z","iopub.execute_input":"2025-08-24T06:09:27.202485Z","iopub.status.idle":"2025-08-24T06:09:27.219146Z","shell.execute_reply.started":"2025-08-24T06:09:27.202460Z","shell.execute_reply":"2025-08-24T06:09:27.218466Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"**DataSet**","metadata":{}},{"cell_type":"code","source":"class CarDataset(Dataset):\n    def __init__(self, images_dir, annotations_csv, img_size=416, transforms=None):\n\n        #annotations_csv  columns: image, xmin, ymin, xmax, ymax\n        \n        self.images_dir = Path(images_dir)\n        self.df = pd.read_csv(annotations_csv)\n        # Keep only bounding boxes with non-zero area\n        self.df = self.df[self.df['xmax'] > self.df['xmin']]\n        # Group by image\n        grouped = defaultdict(list)\n        for _, row in self.df.iterrows():\n            img = row['image']\n            xmin, ymin, xmax, ymax = row['xmin'], row['ymin'], row['xmax'], row['ymax']\n            grouped[img].append([xmin, ymin, xmax, ymax, 0])  # class 0 = car\n        self.image_list = list(grouped.keys())\n        self.boxes = grouped\n        self.img_size = img_size\n        self.transforms = transforms\n\n    def __len__(self):\n        return len(self.image_list)\n\n    def __getitem__(self, idx):\n        img_name = self.image_list[idx]\n        img_path = self.images_dir / img_name\n        img = cv2.imread(str(img_path))\n        if img is None:\n            # try with jpg or png fix\n            raise FileNotFoundError(f\"Image not found: {img_path}\")\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        H0, W0 = img.shape[:2]\n        # resize, keep it simple (YOLO commonly uses 416)\n        img = cv2.resize(img, (self.img_size, self.img_size))\n        # normalize to [0,1]\n        img = img.astype(np.float32) / 255.0\n        # HWC -> CHW\n        img_tensor = torch.tensor(img).permute(2, 0, 1)\n\n        # convert boxes to normalized cx,cy,w,h relative to resized image\n        boxes = np.array(self.boxes[img_name], dtype=np.float32)  # Nx5 (xmin,ymin,xmax,ymax,class)\n        # convert according to original -> resized (but we resized ignoring aspect ratio)\n        scale_x = self.img_size / W0\n        scale_y = self.img_size / H0\n        boxes[:, [0,2]] *= scale_x\n        boxes[:, [1,3]] *= scale_y\n        # convert to normalized cx,cy,w,h (0..1)\n        x_centers = (boxes[:,0] + boxes[:,2]) / 2.0 / self.img_size\n        y_centers = (boxes[:,1] + boxes[:,3]) / 2.0 / self.img_size\n        widths = (boxes[:,2] - boxes[:,0]) / self.img_size\n        heights = (boxes[:,3] - boxes[:,1]) / self.img_size\n        classes = boxes[:,4].astype(np.int64)\n        target = np.stack([classes, x_centers, y_centers, widths, heights], axis=1)  # (N,5)\n\n        # If transforms requested (e.g., random flip) apply here - omitted for simplicity\n        return img_tensor, torch.tensor(target, dtype=torch.float32), img_name\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:10:47.999539Z","iopub.execute_input":"2025-08-24T06:10:48.000210Z","iopub.status.idle":"2025-08-24T06:10:48.009287Z","shell.execute_reply.started":"2025-08-24T06:10:48.000183Z","shell.execute_reply":"2025-08-24T06:10:48.008662Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Non Max Suppression wrapper\ndef nms_boxes(boxes_xywh, scores, iou_thresh=0.5, score_thresh=0.3):\n    # boxes_xywh in normalized cx,cy,w,h -> convert to x1,y1,x2,y2 in absolute (0..1)\n    cx = boxes_xywh[:,0]\n    cy = boxes_xywh[:,1]\n    w = boxes_xywh[:,2]\n    h = boxes_xywh[:,3]\n    x1 = (cx - w/2)\n    y1 = (cy - h/2)\n    x2 = (cx + w/2)\n    y2 = (cy + h/2)\n    boxes_xyxy = torch.stack([x1,y1,x2,y2], dim=1)\n    keep = ops.nms(boxes_xyxy, scores, iou_thresh)\n    return keep, boxes_xyxy","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:10:52.897419Z","iopub.execute_input":"2025-08-24T06:10:52.898128Z","iopub.status.idle":"2025-08-24T06:10:52.902560Z","shell.execute_reply.started":"2025-08-24T06:10:52.898103Z","shell.execute_reply":"2025-08-24T06:10:52.901903Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def cellboxes_to_boxes(output, anchors, S):\n    device = output.device\n    B, A, H, W, C = output.shape\n    outputs = output.clone()\n\n    grid_y, grid_x = torch.meshgrid(torch.arange(S), torch.arange(S), indexing=\"ij\")\n    grid_x, grid_y = grid_x.to(device), grid_y.to(device)\n\n    cx = (outputs[..., 0] + grid_x[None,None,...]) / W\n    cy = (outputs[..., 1] + grid_y[None,None,...]) / H\n\n    # anchors expected shape: (3, 2)\n    pw = (torch.exp(outputs[..., 2]) * anchors[:, 0].view(1, A, 1, 1)) / S\n    ph = (torch.exp(outputs[..., 3]) * anchors[:, 1].view(1, A, 1, 1)) / S\n\n    # Convert cx,cy,pw,ph → x1,y1,x2,y2\n    x1 = cx - pw / 2\n    y1 = cy - ph / 2\n    x2 = cx + pw / 2\n    y2 = cy + ph / 2\n\n    boxes = torch.stack([x1, y1, x2, y2], dim=-1)\n    obj = outputs[..., 4]\n    cls_probs = outputs[..., 5:]\n\n    return boxes, obj, cls_probs\n\n\n   ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:46:50.267634Z","iopub.execute_input":"2025-08-24T06:46:50.268142Z","iopub.status.idle":"2025-08-24T06:46:50.274393Z","shell.execute_reply.started":"2025-08-24T06:46:50.268118Z","shell.execute_reply":"2025-08-24T06:46:50.273720Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nIMG_SIZE = 416\nBATCH_SIZE = 8\nNUM_WORKERS = 4\nNUM_CLASSES = 1  # car only\nLEARNING_RATE = 1e-4\nEPOCHS = 20\nSAVE_DIR = Path( \"/kaggle/working/yolov3_epoch1.pth\")\nSAVE_DIR.mkdir(exist_ok=True)\n\n# Default YOLOv3 anchors (width,height) on the original scale (pixels) (commonly used)\nANCHORS = [\n    [(116,90), (156,198), (373,326)],  # large scale (S=32)\n    [(30,61), (62,45), (59,119)],      # medium scale (S=16)\n    [(10,13), (16,30), (33,23)],       # small scale (S=8)\n]\nSCALES = [32, 16, 8] ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:12:00.244454Z","iopub.execute_input":"2025-08-24T06:12:00.245406Z","iopub.status.idle":"2025-08-24T06:12:00.250791Z","shell.execute_reply.started":"2025-08-24T06:12:00.245374Z","shell.execute_reply":"2025-08-24T06:12:00.249968Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**Loss**","metadata":{}},{"cell_type":"code","source":"class SimpleYOLOLoss(nn.Module):\n    def __init__(self, anchors, S, lambda_box=5, lambda_noobj=0.5):\n        super().__init__()\n        self.mse = nn.MSELoss(reduction='sum')\n        self.bce = nn.BCEWithLogitsLoss(reduction='sum')  # operate on raw logits\n        self.anchors = anchors\n        self.S = S\n        self.lambda_box = lambda_box\n        self.lambda_noobj = lambda_noobj\n\n    def forward(self, outputs, targets):\n        \n        device = outputs[0].device\n        batch_size = outputs[0].shape[0]\n        total_loss = torch.tensor(0., device=device)\n        loss_box = 0.\n        loss_obj = 0.\n        loss_noobj = 0.\n        loss_cls = 0.\n\n        # For simplicity: for every scale, treat each cell and anchor as prediction.\n        # We'll create target tensors per output that are zeros and fill cells that correspond to GTs.\n        for scale_i, out in enumerate(outputs):\n            B, A, H, W, C = out.shape\n            out = out.view(B, A, H, W, C)\n            stride = SCALES[scale_i]\n            anchor_set = torch.tensor(ANCHORS[scale_i], dtype=torch.float32).to(device)\n            # build target tensors\n            t_cls = torch.zeros_like(out[..., 5:])\n            t_obj = torch.zeros_like(out[..., 4])\n            t_box = torch.zeros_like(out[..., :4])\n            # For each image in batch, assign GTs naively:\n            for b in range(B):\n                gt = targets[b]  # Nx5: (class, cx, cy, w, h) normalized\n                if gt.numel() == 0:\n                    continue\n                for g in gt:\n                    cls_id = int(g[0].item())\n                    cx, cy, w_norm, h_norm = g[1].item(), g[2].item(), g[3].item(), g[4].item()\n                    # find which cell this gt falls into at this scale\n                    cell_x = int(cx * W)\n                    cell_y = int(cy * H)\n                    if cell_x < 0 or cell_x >= W or cell_y < 0 or cell_y >= H:\n                        continue\n                    # choose best anchor by IoU with gt (anchor w/h scaled by 1/S)\n                    # convert anchor dims to normalized by dividing by IMG_SIZE\n                    anchors_norm = anchor_set / IMG_SIZE\n                    gt_box = torch.tensor([0.,0.,w_norm,h_norm], device=device).unsqueeze(0)\n                    anchors_box = torch.cat([torch.zeros_like(anchors_norm).unsqueeze(1), anchors_norm.unsqueeze(1)], dim=1)\n                    # compute simple IoU on w,h\n                    inter_w = torch.min(anchors_norm[:, 0], torch.tensor(w_norm, device=anchors_norm.device))\n                    inter_h = torch.min(anchors_norm[:, 1], torch.tensor(h_norm, device=anchors_norm.device))\n\n                    inter = inter_w * inter_h\n                    union = (anchors_norm[:,0]*anchors_norm[:,1]) + (w_norm*h_norm) - inter + 1e-6\n                    ious = inter / union\n                    best_anchor = torch.argmax(ious).item()\n                    # set objectness target\n                    t_obj[b, best_anchor, cell_y, cell_x] = 1.0\n                    # set class target\n                    t_cls[b, best_anchor, cell_y, cell_x, cls_id] = 1.0\n                    # box target: tx,ty,tw,th relative to cell\n                    # tx = cx_cell - cell_x; ty = cy_cell - cell_y\n                    tx = cx * W - cell_x\n                    ty = cy * H - cell_y\n                    # tw = log(w / anchor_w_norm) in YOLOv3, but here we will store normalized values\n                    t_box[b, best_anchor, cell_y, cell_x, 0] = tx\n                    t_box[b, best_anchor, cell_y, cell_x, 1] = ty\n                    # tw/th store sqrt of w/h to stabilize (some tutorials do)\n                    t_box[b, best_anchor, cell_y, cell_x, 2] = math.log((w_norm + 1e-6) / (anchors_norm[best_anchor,0]+1e-6))\n                    t_box[b, best_anchor, cell_y, cell_x, 3] = math.log((h_norm + 1e-6) / (anchors_norm[best_anchor,1]+1e-6))\n\n            # losses:\n            # objectness: BCEWithLogits comparing out[...,4] to t_obj\n            pred_obj = out[..., 4]\n            loss_obj_scale = self.bce(pred_obj, t_obj)\n            # no-object: for cells where t_obj==0, penalize predictions\n            loss_noobj_scale = self.bce(pred_obj, t_obj)  # simple; could weight by mask\n\n            # box regression: MSE on coordinates and log-space wh\n            loss_box_scale = self.mse(out[..., :4] * t_obj.unsqueeze(-1), t_box * t_obj.unsqueeze(-1))\n\n            # class loss\n            pred_cls = out[..., 5:]\n            loss_cls_scale = self.mse(pred_cls * t_obj.unsqueeze(-1), t_cls * t_obj.unsqueeze(-1))\n\n            loss_obj += loss_obj_scale\n            loss_noobj += loss_noobj_scale\n            loss_box += loss_box_scale\n            loss_cls += loss_cls_scale\n\n        total_loss = loss_box * self.lambda_box + loss_obj + self.lambda_noobj * loss_noobj + loss_cls\n        # normalize by batch\n        total_loss = total_loss / max(1, batch_size)\n        return total_loss\n                    \n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:12:06.327173Z","iopub.execute_input":"2025-08-24T06:12:06.327523Z","iopub.status.idle":"2025-08-24T06:12:06.339979Z","shell.execute_reply.started":"2025-08-24T06:12:06.327502Z","shell.execute_reply":"2025-08-24T06:12:06.339247Z"}},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":"**Training**","metadata":{}},{"cell_type":"code","source":"def collate_fn(batch):\n    imgs = torch.stack([b[0] for b in batch], dim=0)\n    targets = [b[1] for b in batch]\n    names = [b[2] for b in batch]\n    return imgs, targets, names\n\ndef train():\n    # paths - adjust if your mount point differs\n    train_images_dir = \"/kaggle/input/car-object-detection/data/training_images\"\n    annotations_csv = \"/kaggle/input/car-object-detection/data/train_solution_bounding_boxes (1).csv\"\n\n    dataset = CarDataset(train_images_dir, annotations_csv, img_size=IMG_SIZE)\n    loader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn, num_workers=NUM_WORKERS)\n\n    model = YOLOv3(in_channels=3, num_classes=NUM_CLASSES).to(DEVICE)\n    opt = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    criterion = SimpleYOLOLoss(ANCHORS, SCALES)\n\n    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n\n    for epoch in range(1, EPOCHS+1):\n        model.train()\n        pbar = tqdm(loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n        epoch_loss = 0.0\n        for imgs, targets, names in pbar:\n            imgs = imgs.to(DEVICE)\n            # targets is a list of variable sized tensors. We'll pack into a padded tensor for simplicity:\n            # convert list to a padded tensor of shape (B, maxN, 5)\n            maxN = max([t.shape[0] for t in targets]) if len(targets)>0 else 1\n            padded = torch.zeros((imgs.shape[0], maxN, 5), device=DEVICE)\n            for i,t in enumerate(targets):\n                if t.numel() == 0:\n                    continue\n                padded[i,:t.shape[0],:] = t.to(DEVICE)\n\n            opt.zero_grad()\n            with torch.cuda.amp.autocast() if scaler is not None else torch.no_grad():\n                outputs = model(imgs)  # list of 3 outputs\n                loss = criterion(outputs, padded)  # padded used as per-batch targets\n            if scaler is not None:\n                scaler.scale(loss).backward()\n                scaler.step(opt)\n                scaler.update()\n            else:\n                loss.backward()\n                opt.step()\n            epoch_loss += loss.item()\n            pbar.set_postfix({\"loss\": loss.item()})\n        avg_loss = epoch_loss / len(loader)\n        print(f\"Epoch {epoch} finished. Avg loss: {avg_loss:.4f}\")\n        torch.save(model.state_dict(), SAVE_DIR / f\"yolov3_epoch{epoch}.pth\")\n     \n    print(\"Training finished. Models saved to:\", SAVE_DIR)\n\n# ---------- Entry ----------\nif __name__ == \"__main__\":\n    print(\"Device:\", DEVICE)\n    # Train\n    train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-24T06:14:02.125229Z","iopub.execute_input":"2025-08-24T06:14:02.126008Z","iopub.status.idle":"2025-08-24T06:18:09.665513Z","shell.execute_reply.started":"2025-08-24T06:14:02.125979Z","shell.execute_reply":"2025-08-24T06:18:09.664761Z"}},"outputs":[{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/932712997.py:19: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\nEpoch 1/20:   0%|          | 0/45 [00:00<?, ?it/s]/tmp/ipykernel_36/932712997.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast() if scaler is not None else torch.no_grad():\nEpoch 1/20: 100%|██████████| 45/45 [00:11<00:00,  3.81it/s, loss=4.21e+3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1 finished. Avg loss: 8663.4695\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/20: 100%|██████████| 45/45 [00:12<00:00,  3.74it/s, loss=2.92e+3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2 finished. Avg loss: 3442.3212\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/20: 100%|██████████| 45/45 [00:12<00:00,  3.73it/s, loss=2.17e+3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3 finished. Avg loss: 2459.8296\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/20: 100%|██████████| 45/45 [00:12<00:00,  3.74it/s, loss=1.72e+3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4 finished. Avg loss: 1911.4887\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/20: 100%|██████████| 45/45 [00:12<00:00,  3.74it/s, loss=1.36e+3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5 finished. Avg loss: 1503.3826\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/20: 100%|██████████| 45/45 [00:11<00:00,  3.78it/s, loss=1.13e+3]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6 finished. Avg loss: 1212.3729\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/20: 100%|██████████| 45/45 [00:11<00:00,  3.80it/s, loss=934]    \n","output_type":"stream"},{"name":"stdout","text":"Epoch 7 finished. Avg loss: 997.8733\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/20: 100%|██████████| 45/45 [00:11<00:00,  3.78it/s, loss=764]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8 finished. Avg loss: 831.1090\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/20: 100%|██████████| 45/45 [00:11<00:00,  3.81it/s, loss=686]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9 finished. Avg loss: 703.1289\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/20: 100%|██████████| 45/45 [00:12<00:00,  3.68it/s, loss=559]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10 finished. Avg loss: 606.2426\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/20: 100%|██████████| 45/45 [00:12<00:00,  3.73it/s, loss=504]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11 finished. Avg loss: 524.6191\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/20: 100%|██████████| 45/45 [00:11<00:00,  3.79it/s, loss=427]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12 finished. Avg loss: 458.0506\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/20: 100%|██████████| 45/45 [00:11<00:00,  3.78it/s, loss=403]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13 finished. Avg loss: 402.4054\n","output_type":"stream"},{"name":"stderr","text":"Epoch 14/20: 100%|██████████| 45/45 [00:11<00:00,  3.76it/s, loss=352]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14 finished. Avg loss: 356.0790\n","output_type":"stream"},{"name":"stderr","text":"Epoch 15/20: 100%|██████████| 45/45 [00:12<00:00,  3.75it/s, loss=312]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15 finished. Avg loss: 317.6594\n","output_type":"stream"},{"name":"stderr","text":"Epoch 16/20: 100%|██████████| 45/45 [00:12<00:00,  3.74it/s, loss=276]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16 finished. Avg loss: 284.3079\n","output_type":"stream"},{"name":"stderr","text":"Epoch 17/20: 100%|██████████| 45/45 [00:11<00:00,  3.79it/s, loss=256]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17 finished. Avg loss: 256.6249\n","output_type":"stream"},{"name":"stderr","text":"Epoch 18/20: 100%|██████████| 45/45 [00:11<00:00,  3.77it/s, loss=228]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18 finished. Avg loss: 242.2095\n","output_type":"stream"},{"name":"stderr","text":"Epoch 19/20: 100%|██████████| 45/45 [00:11<00:00,  3.78it/s, loss=207]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19 finished. Avg loss: 217.2490\n","output_type":"stream"},{"name":"stderr","text":"Epoch 20/20: 100%|██████████| 45/45 [00:11<00:00,  3.81it/s, loss=188]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20 finished. Avg loss: 196.0594\nTraining finished. Models saved to: /kaggle/working/yolov3_epoch1.pth\n","output_type":"stream"}],"execution_count":26}]}