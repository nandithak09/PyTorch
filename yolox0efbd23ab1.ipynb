{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\n# Basic Conv + BN + SiLU block\n\nclass ConvBNAct(nn.Module):\n    def __init__(self, in_channels, out_channels, k=1, s=1, p=0):\n        super().__init__()\n        self.conv = nn.Conv2d(in_channels, out_channels, k, s, p, bias=False)\n        self.bn = nn.BatchNorm2d(out_channels)\n        self.act = nn.SiLU(inplace=True)\n\n    def forward(self, x):\n        return self.act(self.bn(self.conv(x)))\n\n\n# CSP block \nclass CSPBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        mid = out_channels // 2\n        self.conv1 = ConvBNAct(in_channels, mid, 1, 1, 0)\n        self.conv2 = ConvBNAct(mid, out_channels, 3, 1, 1)\n        self.conv_merge = ConvBNAct(in_channels + out_channels, out_channels, 1, 1, 0)\n\n    def forward(self, x):\n        x1 = self.conv1(x)\n        y1 = self.conv2(x1)\n        out = torch.cat([x, y1], dim=1)\n        return self.conv_merge(out)\n\n\n# Backbone (CSPDarknet-tiny)\n\nclass CSPDarknetTiny(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.stem = ConvBNAct(3, 32, 3, 1, 1)\n        self.stage2 = nn.Sequential(\n            ConvBNAct(32, 64, 3, 2, 1),\n            CSPBlock(64, 128),   # c2 → 128\n        )\n        self.stage3 = nn.Sequential(\n            ConvBNAct(128, 128, 3, 2, 1),\n            CSPBlock(128, 256),  # c3 → 256\n        )\n        self.stage5 = nn.Sequential(\n            ConvBNAct(256, 256, 3, 2, 1),\n            CSPBlock(256, 512),  # c5 → 512\n        )\n\n    def forward(self, x):\n        x = self.stem(x)\n        c2 = self.stage2(x)\n        c3 = self.stage3(c2)\n        c5 = self.stage5(c3)\n        return c2, c3, c5  \n\n    \nclass PAFPN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n        # Top-down reductions\n        self.reduce_c5 = ConvBNAct(512, 256, 1, 1, 0)\n        self.reduce_c3 = ConvBNAct(256, 128, 1, 1, 0)\n        self.reduce_p5_to_128 = ConvBNAct(256, 128, 1, 1, 0)\n\n        # Top-down outputs\n        self.top_out_c3 = ConvBNAct(128, 128, 3, 1, 1)\n        self.top_out_c2 = ConvBNAct(128, 128, 3, 1, 1)\n\n        # Bottom-up downsampling (channels corrected)\n        self.down_c2 = ConvBNAct(128, 128, 3, 2, 1)    # p3 → n3 input\n        self.down_c3 = ConvBNAct(256, 256, 3, 2, 1)    # n3 → n5 input (fixed!)\n\n        # Bottom-up outputs\n        self.out_c3 = ConvBNAct(128, 256, 3, 1, 1)     # mid-level → 256\n        self.out_c5 = ConvBNAct(256, 512, 3, 1, 1)     # top-level → 512\n\n    def forward(self, feats):\n        c2, c3, c5 = feats\n\n        # Top-down\n        p5 = self.reduce_c5(c5)       # 512→256\n        p4 = self.reduce_c3(c3)       # 256→128\n        p5_upsampled = F.interpolate(p5, size=p4.shape[2:], mode=\"nearest\")\n        p5_upsampled = self.reduce_p5_to_128(p5_upsampled)\n        p4 = p4 + p5_upsampled\n        p4 = self.top_out_c3(p4)\n\n        p3 = c2 + F.interpolate(p4, size=c2.shape[2:], mode=\"nearest\")\n        p3 = self.top_out_c2(p3)      # 128 channels\n\n        # Bottom-up\n        n3 = p4 + self.down_c2(p3)    # 128 + 128 → 128\n        n3 = self.out_c3(n3)          # 256 channels\n\n        n5 = p5 + self.down_c3(n3)    # 256 + 256 → 256\n        n5 = self.out_c5(n5)          # 512 channels\n\n        return [p3, n3, n5]           # [128, 256, 512]\n\n\n        \n        \n# YOLOX Head\n\nclass YOLOXHead(nn.Module):\n    def __init__(self, num_classes=80):\n        super().__init__()\n        self.cls_convs = nn.ModuleList([\n            ConvBNAct(128, 128, 3),  # for out_c2\n            ConvBNAct(256, 256, 3),  # for out_c3\n            ConvBNAct(512, 512, 3)   # for out_c5\n        ])\n        self.reg_convs = nn.ModuleList([\n            ConvBNAct(128, 128, 3),\n            ConvBNAct(256, 256, 3),\n            ConvBNAct(512, 512, 3)\n        ])\n        self.cls_preds = nn.ModuleList([\n            nn.Conv2d(128, num_classes, 1),\n            nn.Conv2d(256, num_classes, 1),\n            nn.Conv2d(512, num_classes, 1)\n        ])\n        self.obj_preds = nn.ModuleList([\n            nn.Conv2d(128, 1, 1),\n            nn.Conv2d(256, 1, 1),\n            nn.Conv2d(512, 1, 1)\n        ])\n        self.reg_preds = nn.ModuleList([\n            nn.Conv2d(128, 4, 1),\n            nn.Conv2d(256, 4, 1),\n            nn.Conv2d(512, 4, 1)\n        ])\n\n    def forward(self, feats):\n        outputs = []\n        for i, feat in enumerate(feats):\n            cls_feat = self.cls_convs[i](feat)\n            reg_feat = self.reg_convs[i](feat)\n\n            cls_output = self.cls_preds[i](cls_feat)\n            obj_output = self.obj_preds[i](reg_feat)\n            reg_output = self.reg_preds[i](reg_feat)\n\n            out = torch.cat([reg_output, obj_output, cls_output], 1)\n            outputs.append(out)\n        return outputs\n\n\n# Full YOLOX (Backbone + FPN + Head)\n\nclass YOLOX(nn.Module):\n    def __init__(self, num_classes=80):\n        super().__init__()\n        self.backbone = CSPDarknetTiny()\n        self.fpn = PAFPN()\n        self.head = YOLOXHead(num_classes)\n\n    def forward(self, x):\n        feats = self.backbone(x)\n        fpn_outs = self.fpn(feats)\n        outputs = self.head(fpn_outs)\n        return outputs\n\n\n# Test\n\nif __name__ == \"__main__\":\n    model = YOLOX(num_classes=80)\n    x = torch.randn(1, 3, 640, 640)\n    out = model(x)\n    for o in out:\n        print(o.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:20:52.942613Z","iopub.execute_input":"2025-09-21T09:20:52.942915Z","iopub.status.idle":"2025-09-21T09:20:56.169208Z","shell.execute_reply.started":"2025-09-21T09:20:52.942890Z","shell.execute_reply":"2025-09-21T09:20:56.168494Z"}},"outputs":[{"name":"stdout","text":"torch.Size([1, 85, 318, 318])\ntorch.Size([1, 85, 158, 158])\ntorch.Size([1, 85, 78, 78])\n","output_type":"stream"}],"execution_count":36},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import Dataset\nimport os\nimport json\nfrom PIL import Image\nimport torchvision.transforms as T\nimport random\nimport numpy as np\n\nclass COCODataset(Dataset):\n    def __init__(self, img_dir, ann_file, img_size=640, mosaic=True):\n        super().__init__()\n        self.img_dir = img_dir\n        self.img_size = img_size\n        self.mosaic = mosaic\n\n        # Load COCO annotations\n        with open(ann_file) as f:\n            data = json.load(f)\n        self.images = {img['id']: img for img in data['images']}\n        self.annotations = data['annotations']\n\n        # Map image_id -> annotations\n        self.img_to_anns = {}\n        for ann in self.annotations:\n            img_id = ann['image_id']\n            if img_id not in self.img_to_anns:\n                self.img_to_anns[img_id] = []\n            self.img_to_anns[img_id].append(ann)\n        self.ids = list(self.images.keys())\n\n        # Category mapping\n        self.cat2id = {cat['id']: idx for idx, cat in enumerate(data['categories'])}\n        self.num_classes = len(self.cat2id)\n\n        self.transforms = T.Compose([\n            T.Resize((img_size, img_size)),\n            T.ToTensor(),\n            T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n        ])\n\n    def __len__(self):\n        return len(self.ids) #Returns number of images in dataset.\n\n    def load_image_target(self, idx):\n        #Loads the image by id and opens it as RGB.\n        img_id = self.ids[idx]\n        img_info = self.images[img_id]\n        img_path = os.path.join(self.img_dir, img_info['file_name'])\n        img = Image.open(img_path).convert(\"RGB\")\n\n        \n        #Collects all bounding boxes and labels for that image.\n        anns = self.img_to_anns.get(img_id, [])\n        boxes, labels = [], []\n        for ann in anns:\n            x, y, w, h = ann['bbox']\n            boxes.append([x, y, x+w, y+h])\n            labels.append(self.cat2id[ann['category_id']])\n        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0,4))\n        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n\n        target = torch.zeros((boxes.shape[0],5))\n        if boxes.shape[0] > 0:\n            target[:,:4] = boxes\n            target[:,4] = labels\n            #Final target shape: [num_objects, 5].\n        return img, target\n\n    def mosaic_augment(self):\n        \n        indices = [random.randint(0, len(self)-1) for _ in range(4)] #Randomly select 4 images.\n        imgs, targets = [], []\n        for idx in indices:\n            img, target = self.load_image_target(idx)\n            img = np.array(img)\n            imgs.append(img)\n            targets.append(target)\n\n       #A blank canvas, divided into 4 quadrants.\n        mosaic_img = np.zeros((self.img_size*2, self.img_size*2, 3), dtype=np.uint8)\n        positions = [(0,0),(0,self.img_size),(self.img_size,0),(self.img_size,self.img_size)]\n        mosaic_targets = []\n\n        for i,(img,target) in enumerate(zip(imgs,targets)):\n            h,w,_ = img.shape\n            img = np.array(Image.fromarray(img).resize((self.img_size,self.img_size), Image.BILINEAR), dtype=np.uint8)\n            y1,x1 = positions[i]\n            mosaic_img[y1:y1+self.img_size, x1:x1+self.img_size] = img\n\n            # Adjust targets\n            #Scale bounding boxes to resized image size.Shift them according to the image’s new location in the mosaic\n            if target.shape[0] > 0:\n                t = target.clone()\n                t[:,0] = t[:,0]/w*self.img_size + x1\n                t[:,1] = t[:,1]/h*self.img_size + y1\n                t[:,2] = t[:,2]/w*self.img_size + x1\n                t[:,3] = t[:,3]/h*self.img_size + y1\n                mosaic_targets.append(t)\n       #Combine all bounding boxes from the 4 images into one tensor.\n        if mosaic_targets:\n            mosaic_targets = torch.cat(mosaic_targets, dim=0)\n        else:\n            mosaic_targets = torch.zeros((0,5))\n        .\n        # Resize final mosaic back to standard input size\n        #The 2×size mosaic is resized back to img_size × img_size.,Converted to tensor and normalized.\n        mosaic_img = Image.fromarray(mosaic_img).resize((self.img_size,self.img_size), Image.BILINEAR)\n        mosaic_img = T.ToTensor()(mosaic_img)\n        mosaic_img = T.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])(mosaic_img)\n\n        return mosaic_img, mosaic_targets\n\n#50% chance: apply mosaic augmentation.50% chance: load a single image normally with transforms.\n#If you used mosaic for every single sample, the model might not see enough \"normal-looking\" images.\n#So we balance: sometimes show the model a mosaic, sometimes a regular image.\n    def __getitem__(self, idx):\n        if self.mosaic and random.random() < 0.5:\n            img, target = self.mosaic_augment()\n        else:\n            img, target = self.load_image_target(idx)\n            img = self.transforms(img)\n        return img, target\n\n#img: tensor of shape (3, img_size, img_size) (normalized).\n#target: [num_objects, 5] (x1, y1, x2, y2, class).\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:20:56.179815Z","iopub.execute_input":"2025-09-21T09:20:56.180334Z","iopub.status.idle":"2025-09-21T09:20:56.212207Z","shell.execute_reply.started":"2025-09-21T09:20:56.180318Z","shell.execute_reply":"2025-09-21T09:20:56.211698Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\n# CIoU loss\n\ndef ciou_loss(pred_boxes, gt_boxes, eps=1e-7):\n    \"\"\"\n    pred_boxes, gt_boxes: [N,4] format: x1,y1,x2,y2\n    \"\"\"\n    #Checks if there are no predicted boxes (empty tensor).If yes, return 0 as the loss.\n    if pred_boxes.numel() == 0:\n        return torch.tensor(0.0, device=pred_boxes.device)\n\n    # Intersection\n    #Computes the coordinates of the intersection box between prediction and GT.\n    #x1, y1 → top-left of intersection (max of both boxes’ top-left corners).\n    #x2, y2 → bottom-right of intersection (min of both boxes’ bottom-right corners).\n    x1 = torch.max(pred_boxes[:,0], gt_boxes[:,0])\n    y1 = torch.max(pred_boxes[:,1], gt_boxes[:,1])\n    x2 = torch.min(pred_boxes[:,2], gt_boxes[:,2])\n    y2 = torch.min(pred_boxes[:,3], gt_boxes[:,3])\n\n    #Computes intersection area.\n    #(x2 - x1) → width of intersection; (y2 - y1) → height.\n    #.clamp(0) ensures width/height cannot be negative (no intersection → 0 area).\n\n    inter = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n    area_pred = (pred_boxes[:,2]-pred_boxes[:,0])*(pred_boxes[:,3]-pred_boxes[:,1])\n    area_gt   = (gt_boxes[:,2]-gt_boxes[:,0])*(gt_boxes[:,3]-gt_boxes[:,1])\n\n    union = area_pred + area_gt - inter + eps\n    iou = inter / union\n\n    loss = 1 - iou\n    return loss.mean() #Returns the average CIoU loss over all boxes in the batch.\n\n# SimOTA Assigner\n\nclass SimOTAAssigner:\n    def __init__(self, num_classes=80, top_k=10):\n        self.num_classes = num_classes\n        self.top_k = top_k\n\n    def assign(self, pred_boxes, pred_cls, gt_boxes, gt_classes):\n        \"\"\"\n        Returns:\n        unique_pos_idx: indices of positive predictions\n        final_assigned_gt_idx: corresponding GT index for each positive\n        \"\"\"\n        #If there are no predictions or no GT boxes, return empty tensors.\n        num_preds = pred_boxes.shape[0]\n        num_gt = gt_boxes.shape[0]\n\n        if num_gt == 0 or num_preds == 0:\n            return torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)\n\n        # Step 1: IoU matrix [num_preds, num_gt]\n        ious = self.bbox_iou(pred_boxes, gt_boxes)\n\n        # Step 2: top-k candidates per GT\n        pos_idx_list = []\n        assigned_gt_idx_list = []\n\n        #For each GT box:\n          #Take IoU of all predictions with this GT (iou_per_gt).\n          #Pick top-k predictions that best match this GT.\n          #Save their indices in pos_idx_list.\n          #Assign GT index to these predictions in assigned_gt_idx_list\n\n        for gt_idx in range(num_gt):\n            iou_per_gt = ious[:, gt_idx]\n            k = min(self.top_k, num_preds)\n            topk_val, topk_idx = torch.topk(iou_per_gt, k)\n            pos_idx_list.append(topk_idx)\n            assigned_gt_idx_list.append(torch.full_like(topk_idx, gt_idx))\n\n       #Concatenate all top-k candidates across GTs.\n       #Now pos_idx = indices of candidate positive predictions.\n       #assigned_gt_idx = corresponding GT index for each candidate.\n        pos_idx = torch.cat(pos_idx_list)\n        assigned_gt_idx = torch.cat(assigned_gt_idx_list)\n\n        # Step 3: ensure unique predictions → assign GT with max IoU\n        unique_pos_idx = torch.unique(pos_idx)\n        final_assigned_gt_idx = torch.zeros_like(unique_pos_idx)\n\n        for i, p in enumerate(unique_pos_idx):\n            mask = (pos_idx == p)\n            # select GT with maximum IoU\n            gt_candidates = assigned_gt_idx[mask]\n            iou_candidates = ious[p, gt_candidates]\n            final_assigned_gt_idx[i] = gt_candidates[iou_candidates.argmax()]\n\n        return unique_pos_idx, final_assigned_gt_idx\n#unique_pos_idx → final positive prediction indices.\n#final_assigned_gt_idx → which GT each positive prediction is assigned to.\n\n    @staticmethod\n    def bbox_iou(box1, box2):\n        N = box1.shape[0]\n        M = box2.shape[0]\n\n        lt = torch.max(box1[:,None,:2], box2[:,:2])\n        rb = torch.min(box1[:,None,2:], box2[:,2:])\n\n        wh = (rb - lt).clamp(0)\n        inter = wh[:,:,0] * wh[:,:,1]\n\n        area1 = ((box1[:,2]-box1[:,0])*(box1[:,3]-box1[:,1]))[:,None]\n        area2 = ((box2[:,2]-box2[:,0])*(box2[:,3]-box2[:,1]))[None,:]\n\n        iou = inter / (area1 + area2 - inter + 1e-7)\n        return iou\n\n\n# YOLOX Loss\n\nclass YOLOXLoss(nn.Module):\n    def __init__(self, num_classes=80):\n        super().__init__()\n        self.num_classes = num_classes\n        self.assigner = SimOTAAssigner(num_classes)\n\n    def forward(self, preds, targets):\n        \"\"\"\n        preds: list of feature maps at 3 scales [B,4+1+C,H,W]\n        targets: list of tensors [num_objects,5] per image\n        \"\"\"\n        device = preds[0].device\n        loss_cls, loss_obj, loss_reg = 0.0, 0.0, 0.0\n        B = len(targets)\n\n        #Each pred is a feature map.permute → move channel dimension last to [B,H,W,C].reshape → flatten spatial dimensions → [B, H*W, C]\n        #Now every predicted box is a row in pred.\n        for pred in preds:\n            B, C, H, W = pred.shape\n            pred = pred.permute(0,2,3,1).reshape(B, -1, C)\n\n            pred_boxes = pred[...,:4]\n            pred_obj   = pred[...,4:5]\n            pred_cls   = pred[...,5:]\n\n            #For each image in the batch:\n           #gt → GT boxes for this image.\n           #Skip if no GT boxes (empty image).\n            for b in range(B):\n                gt = targets[b]\n                if gt.shape[0] == 0:\n                    continue\n            #Separate GT boxes and classes\n                gt_boxes = gt[:,:4]\n                gt_classes = gt[:,4].long()\n\n                # SimOTA assignment\n                pos_idx, assigned_gt_idx = self.assigner.assign(pred_boxes[b], pred_cls[b], gt_boxes, gt_classes)\n                if pos_idx.numel() == 0:\n                    continue\n\n                # Positive predictions and corresponding GTs\n                pos_pred_boxes = pred_boxes[b][pos_idx]\n                assigned_gt_boxes = gt_boxes[assigned_gt_idx]\n                pos_pred_obj = pred_obj[b][pos_idx]\n                pos_pred_cls = pred_cls[b][pos_idx]\n                assigned_gt_classes = gt_classes[assigned_gt_idx]\n\n                # Regression loss (CIoU)\n                loss_reg += ciou_loss(pos_pred_boxes, assigned_gt_boxes)\n\n                # Objectness loss\n                target_obj = torch.ones_like(pos_pred_obj, device=device)\n                loss_obj += nn.BCEWithLogitsLoss()(pos_pred_obj, target_obj)\n\n                # Classification loss (multi-class BCE)\n                target_cls = torch.zeros_like(pos_pred_cls, device=device)\n                target_cls[range(len(assigned_gt_classes)), assigned_gt_classes] = 1.0\n                loss_cls += nn.BCEWithLogitsLoss()(pos_pred_cls, target_cls)\n\n        total_loss = loss_cls + loss_obj + loss_reg\n        return total_loss, loss_cls, loss_obj, loss_reg\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:20:56.213611Z","iopub.execute_input":"2025-09-21T09:20:56.214146Z","iopub.status.idle":"2025-09-21T09:20:56.247294Z","shell.execute_reply.started":"2025-09-21T09:20:56.214127Z","shell.execute_reply":"2025-09-21T09:20:56.246700Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\n# Make sure these are imported or defined in your project\n# from dataset import COCODatasetYOLOX\n# from model import YOLOX\n# from loss import YOLOXLoss\n\ndef main():\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ----------------------------\n    # Dataset and DataLoader\n    # ----------------------------\n    train_dataset = COCODataset(\n        img_dir='/kaggle/input/2017-2017/train2017/train2017',\n        ann_file='/kaggle/input/2017-2017/annotations_trainval2017/annotations/instances_train2017.json',\n        img_size=640,\n        mosaic=True\n    )\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=2,\n        shuffle=True,\n        num_workers=4,\n        collate_fn=lambda x: tuple(zip(*x))\n    )\n\n    # ----------------------------\n    # Model, Loss, Optimizer\n    # ----------------------------\n    model = YOLOX(num_classes=train_dataset.num_classes).to(device)\n    criterion = YOLOXLoss(num_classes=train_dataset.num_classes)\n    optimizer = torch.optim.SGD(\n        model.parameters(),\n        lr=0.01,\n        momentum=0.9,\n        weight_decay=5e-4\n    )\n\n    num_epochs = 20\n\n    # ----------------------------\n    # Training Loop\n    # ----------------------------\n    for epoch in range(num_epochs):\n        model.train()\n        loop = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n        for imgs, targets in loop:\n            imgs = torch.stack(imgs).to(device)        # [B,3,H,W]\n            targets = [t.to(device) for t in targets]  # list of [N,5]\n\n            optimizer.zero_grad()\n            preds = model(imgs)  # list of feature maps per scale\n            loss, loss_cls, loss_obj, loss_reg = criterion(preds, targets)\n            loss.backward()\n            optimizer.step()\n\n            loop.set_postfix({\n                'loss': loss.item(),\n                'cls': loss_cls.item(),\n                'obj': loss_obj.item(),\n                'reg': loss_reg.item()\n            })\n\n        # Optional: Save checkpoint every epoch\n        torch.save({\n            'epoch': epoch+1,\n            'model_state_dict': model.state_dict(),\n            'optimizer_state_dict': optimizer.state_dict()\n        }, f'yolox_epoch{epoch+1}.pt')\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T09:20:56.247880Z","iopub.execute_input":"2025-09-21T09:20:56.248073Z","execution_failed":"2025-09-21T09:21:30.520Z"}},"outputs":[{"name":"stderr","text":"Epoch 1/20:   0%|          | 11/59144 [00:08<9:51:43,  1.67it/s, loss=6.54, cls=0.535, obj=9.31e-12, reg=6] ","output_type":"stream"}],"execution_count":null}]}